<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gradient Boosting: The Foundation of XGBoost</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/3.9.1/chart.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.7;
            color: #2c3e50;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }
        h1 {
            text-align: center;
            color: #667eea;
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        .subtitle {
            text-align: center;
            color: #7f8c8d;
            margin-bottom: 40px;
            font-size: 1.1em;
        }
        h2 {
            color: #764ba2;
            margin-top: 40px;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }
        h3 {
            color: #34495e;
            margin: 20px 0 15px 0;
        }
        .story-box {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 30px;
            border-radius: 15px;
            margin: 30px 0;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
        }
        .concept-card {
            background: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            box-shadow: 0 5px 20px rgba(0,0,0,0.1);
            border-left: 5px solid #667eea;
            transition: transform 0.3s;
        }
        .concept-card:hover {
            transform: translateX(5px);
        }
        .math-box {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
        }
        .formula-box {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 5px solid #667eea;
            font-family: 'Courier New', monospace;
        }
        .visual-demo {
            background: linear-gradient(135deg, #667eea10, #764ba210);
            padding: 30px;
            border-radius: 15px;
            margin: 30px 0;
            text-align: center;
        }
        .tree-evolution {
            display: flex;
            justify-content: space-around;
            align-items: center;
            margin: 30px 0;
            flex-wrap: wrap;
            gap: 20px;
        }
        .tree-stage {
            background: white;
            padding: 20px;
            border-radius: 15px;
            box-shadow: 0 5px 20px rgba(0,0,0,0.1);
            text-align: center;
            flex: 1;
            min-width: 200px;
            transition: transform 0.3s;
        }
        .tree-stage:hover {
            transform: scale(1.05);
        }
        .stage-number {
            width: 50px;
            height: 50px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 0 auto 15px;
            font-size: 1.5em;
            font-weight: bold;
        }
        .arrow {
            font-size: 2em;
            color: #667eea;
            margin: 0 10px;
        }
        .comparison-section {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 30px 0;
        }
        .comparison-card {
            padding: 25px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }
        .gradient-boost-card {
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        }
        .xgboost-card {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
        }
        .info-box {
            background: #e8f4f8;
            border-left: 5px solid #3498db;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        .warning-box {
            background: #fff3cd;
            border-left: 5px solid #f39c12;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        .success-box {
            background: #d4edda;
            border-left: 5px solid #27ae60;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 12px 30px;
            border-radius: 25px;
            cursor: pointer;
            font-size: 16px;
            margin: 10px;
            transition: all 0.3s;
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }
        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 20px rgba(0,0,0,0.3);
        }
        .interactive-demo {
            background: white;
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            margin: 30px 0;
        }
        .prediction-flow {
            display: flex;
            align-items: center;
            justify-content: space-around;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        .prediction-box {
            padding: 15px;
            border-radius: 10px;
            text-align: center;
            min-width: 120px;
            margin: 10px;
        }
        .actual-value {
            background: #3498db;
            color: white;
        }
        .prediction {
            background: #e74c3c;
            color: white;
        }
        .residual {
            background: #f39c12;
            color: white;
        }
        .improvement {
            background: #27ae60;
            color: white;
        }
        .chart-container {
            width: 100%;
            max-width: 600px;
            margin: 30px auto;
            padding: 20px;
            background: white;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }
        .algorithm-steps {
            counter-reset: step-counter;
        }
        .algorithm-step {
            position: relative;
            padding-left: 50px;
            margin: 20px 0;
            counter-increment: step-counter;
        }
        .algorithm-step::before {
            content: counter(step-counter);
            position: absolute;
            left: 0;
            top: 0;
            width: 35px;
            height: 35px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
        }
        .code-block {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            line-height: 1.5;
        }
        .highlight {
            background: linear-gradient(135deg, #667eea20 0%, #764ba220 100%);
            padding: 3px 8px;
            border-radius: 5px;
            font-weight: bold;
        }
        .tabs {
            display: flex;
            gap: 10px;
            margin: 20px 0;
            border-bottom: 2px solid #ecf0f1;
        }
        .tab {
            padding: 12px 24px;
            background: #f8f9fa;
            border: none;
            cursor: pointer;
            border-radius: 10px 10px 0 0;
            transition: all 0.3s;
            font-size: 16px;
        }
        .tab.active {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }
        .tab-content {
            display: none;
            padding: 20px 0;
        }
        .tab-content.active {
            display: block;
        }
        .evolution-timeline {
            position: relative;
            padding: 20px 0;
        }
        .timeline-item {
            display: flex;
            margin: 30px 0;
            position: relative;
        }
        .timeline-date {
            min-width: 100px;
            font-weight: bold;
            color: #667eea;
        }
        .timeline-content {
            flex: 1;
            padding-left: 30px;
            border-left: 3px solid #667eea;
            margin-left: 20px;
        }
        .timeline-dot {
            position: absolute;
            width: 20px;
            height: 20px;
            background: #667eea;
            border-radius: 50%;
            left: 110px;
            top: 5px;
            border: 3px solid white;
            box-shadow: 0 2px 5px rgba(0,0,0,0.2);
        }
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.1); }
            100% { transform: scale(1); }
        }
        .animate-pulse {
            animation: pulse 2s infinite;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üöÄ From Gradient Boosting to XGBoost</h1>
        <p class="subtitle">Understanding the Evolution of Boosting Algorithms</p>

        <div class="story-box">
            <h2 style="color: white; border: none;">üìñ The Story: A Master Chef Teaching Apprentices</h2>
            <p>Imagine a master chef (you) teaching a series of apprentice chefs to recreate your signature dish:</p>
            <ul style="margin: 15px 0;">
                <li><strong>Apprentice 1:</strong> Makes a basic version - tastes okay but missing something</li>
                <li><strong>Apprentice 2:</strong> Doesn't try to cook the whole dish - just adds what's missing (more salt, perhaps)</li>
                <li><strong>Apprentice 3:</strong> Adds what's still missing (maybe some herbs)</li>
                <li><strong>Continue...</strong> Each apprentice only learns to fix what previous ones missed</li>
            </ul>
            <p style="margin-top: 15px;"><strong>Final dish = Apprentice 1 + Apprentice 2's additions + Apprentice 3's additions + ...</strong></p>
            <p>This is exactly how Gradient Boosting works! Each "apprentice" (tree) learns to correct the mistakes of all previous ones.</p>
        </div>

        <h2>üéØ What is Gradient Boosting?</h2>
        <div class="concept-card">
            <h3>Core Definition</h3>
            <p><strong>Gradient Boosting</strong> is an ensemble technique that builds models sequentially, where each new model attempts to correct the errors made by the previous ones.</p>
            
            <div class="info-box">
                <strong>Key Insight:</strong> Instead of trying to predict the target directly, each new model predicts the <em>residuals</em> (errors) of all previous models combined.
            </div>
        </div>

        <h2>üî¨ How Gradient Boosting Works - Step by Step</h2>
        <div class="interactive-demo">
            <h3>Example: Predicting House Prices</h3>
            <p>Let's predict the price of a house worth $500,000:</p>
            
            <div class="tree-evolution">
                <div class="tree-stage">
                    <div class="stage-number">1</div>
                    <h4>Initial Prediction</h4>
                    <p><strong>F‚ÇÄ = $400,000</strong></p>
                    <p>(Average price)</p>
                    <p style="color: #e74c3c;">Error: $100,000</p>
                </div>
                
                <span class="arrow">‚Üí</span>
                
                <div class="tree-stage">
                    <div class="stage-number">2</div>
                    <h4>Tree 1: Learn Residuals</h4>
                    <p>Target: $100,000 (error)</p>
                    <p>Predicts: $80,000</p>
                    <p style="color: #f39c12;">New total: $480,000</p>
                </div>
                
                <span class="arrow">‚Üí</span>
                
                <div class="tree-stage">
                    <div class="stage-number">3</div>
                    <h4>Tree 2: Learn Remaining</h4>
                    <p>Target: $20,000 (error)</p>
                    <p>Predicts: $15,000</p>
                    <p style="color: #27ae60;">Final: $495,000</p>
                </div>
            </div>
            
            <button onclick="animateGradientBoosting()">Animate the Process</button>
            <div id="animationResult"></div>
        </div>

        <h2>‚öôÔ∏è The Mathematics Behind Gradient Boosting</h2>
        <div class="formula-box">
            <h3>The Gradient Boosting Algorithm</h3>
            <p><strong>1. Initialize with constant:</strong></p>
            <p>F‚ÇÄ(x) = argmin Œ£ L(y·µ¢, Œ≥)</p>
            
            <p style="margin-top: 15px;"><strong>2. For m = 1 to M (number of trees):</strong></p>
            <p>  a. Compute residuals (negative gradients):</p>
            <p>     r·µ¢‚Çò = -[‚àÇL(y·µ¢, F(x·µ¢))/‚àÇF(x·µ¢)]</p>
            <p>  b. Fit a tree h‚Çò to residuals r·µ¢‚Çò</p>
            <p>  c. Update model:</p>
            <p>     F‚Çò(x) = F‚Çò‚Çã‚ÇÅ(x) + Œ∑ √ó h‚Çò(x)</p>
            
            <p style="margin-top: 15px;"><strong>3. Final prediction:</strong></p>
            <p>F(x) = F‚ÇÄ(x) + Œ£(Œ∑ √ó h‚Çò(x))</p>
        </div>

        <div class="concept-card">
            <h3>üéØ Why "Gradient"?</h3>
            <p>The name comes from the fact that we're using <strong>gradient descent</strong> in function space:</p>
            <ul>
                <li>In regular gradient descent: We update parameters in the direction of negative gradient</li>
                <li>In gradient boosting: We add trees that point in the direction of negative gradient</li>
            </ul>
            
            <div class="info-box">
                <p><strong>Intuition:</strong> Each tree is like taking a step down the steepest slope of the loss function mountain!</p>
            </div>
        </div>

        <h2>üìä Gradient Boosting vs Random Forest</h2>
        <div class="comparison-section">
            <div class="comparison-card gradient-boost-card">
                <h3>üîÑ Gradient Boosting</h3>
                <ul>
                    <li><strong>Sequential:</strong> Trees built one after another</li>
                    <li><strong>Dependent:</strong> Each tree learns from previous errors</li>
                    <li><strong>Depth:</strong> Usually shallow trees (depth 3-8)</li>
                    <li><strong>Focus:</strong> Bias reduction</li>
                    <li><strong>Risk:</strong> Can overfit if not careful</li>
                </ul>
            </div>
            
            <div class="comparison-card xgboost-card">
                <h3>üå≤ Random Forest</h3>
                <ul>
                    <li><strong>Parallel:</strong> Trees built independently</li>
                    <li><strong>Independent:</strong> Each tree sees random data subset</li>
                    <li><strong>Depth:</strong> Usually deep trees</li>
                    <li><strong>Focus:</strong> Variance reduction</li>
                    <li><strong>Risk:</strong> Less prone to overfitting</li>
                </ul>
            </div>
        </div>

        <div class="chart-container">
            <canvas id="comparisonChart"></canvas>
        </div>

        <h2>üîÑ The Evolution: Gradient Boosting ‚Üí XGBoost</h2>
        <div class="evolution-timeline">
            <div class="timeline-item">
                <div class="timeline-date">2001</div>
                <div class="timeline-dot"></div>
                <div class="timeline-content">
                    <h4>Gradient Boosting Machine (GBM)</h4>
                    <p>Jerome Friedman introduces gradient boosting</p>
                    <ul>
                        <li>Basic sequential boosting</li>
                        <li>Effective but slow</li>
                        <li>No regularization</li>
                    </ul>
                </div>
            </div>
            
            <div class="timeline-item">
                <div class="timeline-date">2014</div>
                <div class="timeline-dot"></div>
                <div class="timeline-content">
                    <h4>XGBoost Released</h4>
                    <p>Tianqi Chen creates XGBoost</p>
                    <ul>
                        <li>Adds regularization</li>
                        <li>Parallel processing</li>
                        <li>Handles missing values</li>
                        <li>Tree pruning</li>
                    </ul>
                </div>
            </div>
            
            <div class="timeline-item">
                <div class="timeline-date">2016</div>
                <div class="timeline-dot"></div>
                <div class="timeline-content">
                    <h4>LightGBM</h4>
                    <p>Microsoft's response</p>
                    <ul>
                        <li>Leaf-wise growth</li>
                        <li>Even faster</li>
                    </ul>
                </div>
            </div>
            
            <div class="timeline-item">
                <div class="timeline-date">2017</div>
                <div class="timeline-dot"></div>
                <div class="timeline-content">
                    <h4>CatBoost</h4>
                    <p>Yandex's contribution</p>
                    <ul>
                        <li>Better categorical handling</li>
                        <li>Ordered boosting</li>
                    </ul>
                </div>
            </div>
        </div>

        <h2>üöÄ How XGBoost Improves on Gradient Boosting</h2>
        
        <div class="tabs">
            <button class="tab active" onclick="showTab('improvements')">Key Improvements</button>
            <button class="tab" onclick="showTab('technical')">Technical Details</button>
            <button class="tab" onclick="showTab('comparison')">Side-by-Side Code</button>
        </div>

        <div id="improvements" class="tab-content active">
            <div class="concept-card">
                <h3>1. üõ°Ô∏è Regularization</h3>
                <div class="formula-box">
                    <strong>Gradient Boosting Objective:</strong> L(y, F(x))<br>
                    <strong>XGBoost Objective:</strong> L(y, F(x)) + Œ©(f)<br><br>
                    Where Œ©(f) = Œ≥T + ¬ΩŒª||w||¬≤
                </div>
                <p>XGBoost adds L1 and L2 regularization to prevent overfitting.</p>
            </div>

            <div class="concept-card">
                <h3>2. ‚ö° Second-Order Optimization</h3>
                <p><strong>Gradient Boosting:</strong> Uses only first-order derivatives (gradients)</p>
                <p><strong>XGBoost:</strong> Uses second-order derivatives (Hessians) for better approximation</p>
                
                <div class="info-box">
                    <p>Think of it like this: GB knows the slope, XGBoost knows both the slope AND the curvature!</p>
                </div>
            </div>

            <div class="concept-card">
                <h3>3. üèÉ System Optimizations</h3>
                <ul>
                    <li><strong>Parallelization:</strong> XGBoost parallelizes split finding</li>
                    <li><strong>Cache awareness:</strong> Optimized data structures</li>
                    <li><strong>Out-of-core computing:</strong> Handles data that doesn't fit in memory</li>
                    <li><strong>Sparsity awareness:</strong> Efficient handling of missing values</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>4. üå≤ Tree Building Differences</h3>
                <p><strong>GB:</strong> Grows trees depth-first, stops early</p>
                <p><strong>XGBoost:</strong> Grows to max_depth, then prunes (better trees)</p>
            </div>
        </div>

        <div id="technical" class="tab-content">
            <div class="math-box">
                <h3>Taylor Expansion in XGBoost</h3>
                <p>XGBoost uses second-order Taylor expansion:</p>
                <p style="margin: 15px 0;">
                L(y·µ¢, F(x·µ¢) + f‚Çò(x·µ¢)) ‚âà L(y·µ¢, F(x·µ¢)) + g·µ¢f‚Çò(x·µ¢) + ¬Ωh·µ¢f‚Çò¬≤(x·µ¢)
                </p>
                <p>Where:</p>
                <ul style="list-style: none;">
                    <li>g·µ¢ = ‚àÇL/‚àÇF (first derivative - gradient)</li>
                    <li>h·µ¢ = ‚àÇ¬≤L/‚àÇF¬≤ (second derivative - Hessian)</li>
                </ul>
                
                <p style="margin-top: 20px;"><strong>This leads to optimal leaf weight:</strong></p>
                <p>w‚±º* = -Œ£g·µ¢ / (Œ£h·µ¢ + Œª)</p>
            </div>

            <div class="warning-box">
                <h3>Why Second-Order Matters</h3>
                <p>Using Hessian (second derivative) provides:</p>
                <ul>
                    <li>Better step size estimation</li>
                    <li>Faster convergence</li>
                    <li>More stable optimization</li>
                </ul>
            </div>
        </div>

        <div id="comparison" class="tab-content">
            <div class="code-block">
# Traditional Gradient Boosting (sklearn)
from sklearn.ensemble import GradientBoostingClassifier

# Basic GB with limited options
gb_model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
gb_model.fit(X_train, y_train)

# No built-in:
# - Regularization parameters
# - Missing value handling
# - Parallel processing
# - GPU support
            </div>

            <div class="code-block" style="margin-top: 20px;">
# XGBoost - Enhanced Gradient Boosting
import xgboost as xgb

# XGBoost with advanced features
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    
    # XGBoost additions:
    reg_alpha=1.0,        # L1 regularization
    reg_lambda=2.0,       # L2 regularization
    gamma=0.1,            # Min split loss reduction
    subsample=0.8,        # Row sampling
    colsample_bytree=0.8, # Column sampling
    
    # Performance features:
    tree_method='hist',   # Fast histogram algorithm
    device='cuda',        # GPU support
    n_jobs=-1,           # Parallel processing
    
    # Advanced features:
    scale_pos_weight=3,   # Handle imbalanced data
    missing=np.nan,       # Automatic missing value handling
    random_state=42
)

# Built-in cross-validation
xgb.cv(params, dtrain, num_boost_round=100, nfold=5)
            </div>
        </div>

        <h2>üìà Visual Comparison: Training Process</h2>
        <div class="visual-demo">
            <h3>See the Difference in Action</h3>
            <button onclick="compareTraining()">Compare Training Process</button>
            <div id="trainingComparison"></div>
        </div>

        <div class="chart-container">
            <canvas id="evolutionChart"></canvas>
        </div>

        <h2>üéØ Real Example: Credit Card Fraud Detection</h2>
        <div class="concept-card">
            <h3>Comparing Performance</h3>
            
            <div class="prediction-flow">
                <div class="prediction-box actual-value">
                    <strong>Dataset</strong><br>
                    284,807 transactions<br>
                    492 frauds (0.17%)
                </div>
                
                <div class="prediction-box prediction">
                    <strong>Gradient Boosting</strong><br>
                    AUC: 0.92<br>
                    Time: 45 seconds
                </div>
                
                <div class="prediction-box improvement">
                    <strong>XGBoost</strong><br>
                    AUC: 0.96<br>
                    Time: 12 seconds
                </div>
            </div>
            
            <div class="info-box">
                <strong>Why XGBoost Won:</strong>
                <ul>
                    <li>Regularization prevented overfitting on rare fraud cases</li>
                    <li>Missing value handling (some transactions had incomplete data)</li>
                    <li>3.75x faster training through parallelization</li>
                    <li>scale_pos_weight handled class imbalance</li>
                </ul>
            </div>
        </div>

        <h2>üî¨ Detailed Algorithm Comparison</h2>
        <div class="comparison-section">
            <div class="comparison-card gradient-boost-card">
                <h3>Gradient Boosting Algorithm</h3>
                <div class="algorithm-steps">
                    <div class="algorithm-step">Start with initial prediction (usually mean)</div>
                    <div class="algorithm-step">Calculate residuals (actual - predicted)</div>
                    <div class="algorithm-step">Fit a shallow tree to residuals</div>
                    <div class="algorithm-step">Add tree √ó learning_rate to ensemble</div>
                    <div class="algorithm-step">Repeat until n_estimators reached</div>
                </div>
            </div>
            
            <div class="comparison-card xgboost-card">
                <h3>XGBoost Algorithm</h3>
                <div class="algorithm-steps">
                    <div class="algorithm-step">Start with initial prediction</div>
                    <div class="algorithm-step">Calculate gradients AND Hessians</div>
                    <div class="algorithm-step">Build tree using exact or approximate split finding</div>
                    <div class="algorithm-step">Apply L1/L2 regularization to leaf weights</div>
                    <div class="algorithm-step">Prune tree if gain < gamma</div>
                    <div class="algorithm-step">Add tree with optimal learning rate</div>
                    <div class="algorithm-step">Handle missing values with learned default direction</div>
                </div>
            </div>
        </div>

        <h2>üí° Key Takeaways</h2>
        <div class="success-box">
            <h3>Understanding the Journey</h3>
            <ol>
                <li><strong>Gradient Boosting</strong> = Sequential learning where each tree corrects previous errors</li>
                <li><strong>Uses gradient descent</strong> in function space (hence "gradient" boosting)</li>
                <li><strong>XGBoost</strong> = "eXtreme" version with:
                    <ul>
                        <li>Regularization (prevents overfitting)</li>
                        <li>Second-order optimization (faster, more accurate)</li>
                        <li>Engineering optimizations (parallel processing, cache)</li>
                        <li>Missing value handling</li>
                    </ul>
                </li>
                <li><strong>Result:</strong> XGBoost is typically 10x faster and more accurate than traditional GB</li>
            </ol>
        </div>

        <div class="warning-box">
            <h3>‚ö†Ô∏è Common Misconceptions</h3>
            <ul>
                <li><strong>Myth:</strong> "XGBoost is completely different from Gradient Boosting"<br>
                    <strong>Reality:</strong> XGBoost IS gradient boosting, just optimized and regularized</li>
                <li><strong>Myth:</strong> "More trees always better"<br>
                    <strong>Reality:</strong> Too many trees can overfit; use early stopping</li>
                <li><strong>Myth:</strong> "XGBoost works for everything"<br>
                    <strong>Reality:</strong> Best for tabular data; use deep learning for images/text</li>
            </ul>
        </div>

        <h2>üéì Final Summary</h2>
        <div class="story-box">
            <h3 style="color: white;">The Complete Picture</h3>
            <p><strong>Gradient Boosting</strong> is like teaching a class where each student learns from all previous students' mistakes:</p>
            <ul>
                <li>Student 1: Makes initial attempt</li>
                <li>Student 2: Only focuses on fixing Student 1's errors</li>
                <li>Student 3: Only fixes remaining errors</li>
                <li>Final answer = Sum of all students' contributions</li>
            </ul>
            
            <p style="margin-top: 20px;"><strong>XGBoost</strong> is the same class, but now:</p>
            <ul>
                <li>Students work partially in parallel (faster)</li>
                <li>There's a penalty for overly complex answers (regularization)</li>
                <li>Students use better learning techniques (Newton's method)</li>
                <li>The classroom handles absent students (missing values)</li>
            </ul>
            
            <p style="margin-top: 20px;"><strong>Result:</strong> Much faster, more accurate, and more robust learning!</p>
        </div>
    </div>

    <script>
        // Initialize comparison chart
        const ctx1 = document.getElementById('comparisonChart').getContext('2d');
        const comparisonChart = new Chart(ctx1, {
            type: 'radar',
            data: {
                labels: ['Speed', 'Accuracy', 'Overfitting Resistance', 'Feature Importance', 'Missing Data Handling', 'Ease of Use'],
                datasets: [
                    {
                        label: 'Gradient Boosting',
                        data: [60, 80, 60, 70, 40, 85],
                        borderColor: 'rgba(52, 152, 219, 1)',
                        backgroundColor: 'rgba(52, 152, 219, 0.2)',
                        pointBackgroundColor: 'rgba(52, 152, 219, 1)',
                        pointBorderColor: '#fff',
                        pointHoverBackgroundColor: '#fff',
                        pointHoverBorderColor: 'rgba(52, 152, 219, 1)'
                    },
                    {
                        label: 'XGBoost',
                        data: [95, 95, 90, 85, 95, 80],
                        borderColor: 'rgba(231, 76, 60, 1)',
                        backgroundColor: 'rgba(231, 76, 60, 0.2)',
                        pointBackgroundColor: 'rgba(231, 76, 60, 1)',
                        pointBorderColor: '#fff',
                        pointHoverBackgroundColor: '#fff',
                        pointHoverBorderColor: 'rgba(231, 76, 60, 1)'
                    }
                ]
            },
            options: {
                responsive: true,
                plugins: {
                    title: {
                        display: true,
                        text: 'Gradient Boosting vs XGBoost Comparison',
                        font: { size: 16 }
                    },
                    legend: {
                        position: 'bottom'
                    }
                },
                scales: {
                    r: {
                        beginAtZero: true,
                        max: 100,
                        ticks: {
                            stepSize: 20
                        }
                    }
                }
            }
        });

        // Evolution chart
        const ctx2 = document.getElementById('evolutionChart').getContext('2d');
        const evolutionChart = new Chart(ctx2, {
            type: 'line',
            data: {
                labels: ['Tree 1', 'Tree 5', 'Tree 10', 'Tree 15', 'Tree 20', 'Tree 25', 'Tree 30'],
                datasets: [
                    {
                        label: 'Gradient Boosting Error',
                        data: [0.35, 0.25, 0.18, 0.14, 0.11, 0.09, 0.08],
                        borderColor: '#3498db',
                        backgroundColor: 'rgba(52, 152, 219, 0.1)',
                        tension: 0.4,
                        borderWidth: 2
                    },
                    {
                        label: 'XGBoost Error (with regularization)',
                        data: [0.35, 0.23, 0.15, 0.11, 0.09, 0.08, 0.075],
                        borderColor: '#e74c3c',
                        backgroundColor: 'rgba(231, 76, 60, 0.1)',
                        tension: 0.4,
                        borderWidth: 2
                    }
                ]
            },
            options: {
                responsive: true,
                plugins: {
                    title: {
                        display: true,
                        text: 'Training Error Reduction: GB vs XGBoost',
                        font: { size: 16 }
                    },
                    legend: {
                        position: 'bottom'
                    }
                },
                scales: {
                    y: {
                        beginAtZero: true,
                        title: {
                            display: true,
                            text: 'Error Rate'
                        }
                    },
                    x: {
                        title: {
                            display: true,
                            text: 'Number of Trees'
                        }
                    }
                }
            }
        });

        // Tab functionality
        function showTab(tabName) {
            const tabs = document.querySelectorAll('.tab-content');
            tabs.forEach(tab => tab.classList.remove('active'));
            
            const buttons = document.querySelectorAll('.tab');
            buttons.forEach(btn => btn.classList.remove('active'));
            
            document.getElementById(tabName).classList.add('active');
            event.target.classList.add('active');
        }

        // Animate gradient boosting process
        function animateGradientBoosting() {
            const result = document.getElementById('animationResult');
            result.innerHTML = '<h4>üé¨ Watch the Boosting Process:</h4>';
            
            const steps = [
                { time: 500, text: 'üìä Initial prediction: $400,000 (using average)', value: 400000 },
                { time: 1000, text: '‚ùå Actual price: $500,000 ‚Üí Error: $100,000', error: 100000 },
                { time: 1500, text: 'üå≤ Tree 1 trains on error, predicts: +$80,000', correction: 80000 },
                { time: 2000, text: '‚ú® Updated prediction: $480,000', value: 480000 },
                { time: 2500, text: '‚ùå Remaining error: $20,000', error: 20000 },
                { time: 3000, text: 'üå≤ Tree 2 trains on new error, predicts: +$15,000', correction: 15000 },
                { time: 3500, text: '‚ú® Final prediction: $495,000', value: 495000 },
                { time: 4000, text: '‚úÖ Much closer to actual price! Error reduced by 95%', final: true }
            ];
            
            steps.forEach(step => {
                setTimeout(() => {
                    const div = document.createElement('div');
                    div.className = 'animate-pulse';
                    div.style.padding = '10px';
                    div.style.margin = '5px 0';
                    div.style.borderRadius = '10px';
                    
                    if (step.final) {
                        div.style.background = 'linear-gradient(135deg, #27ae60, #2ecc71)';
                        div.style.color = 'white';
                    } else if (step.error) {
                        div.style.background = '#ffe0e0';
                    } else if (step.correction) {
                        div.style.background = '#fff4e0';
                    } else {
                        div.style.background = '#e0f0ff';
                    }
                    
                    div.innerHTML = step.text;
                    result.appendChild(div);
                }, step.time);
            });
        }

        // Compare training process
        function compareTraining() {
            const result = document.getElementById('trainingComparison');
            result.innerHTML = `
                <div class="comparison-section" style="margin-top: 20px;">
                    <div class="comparison-card gradient-boost-card">
                        <h4>Gradient Boosting Training</h4>
                        <div id="gbProgress"></div>
                    </div>
                    <div class="comparison-card xgboost-card">
                        <h4>XGBoost Training</h4>
                        <div id="xgbProgress"></div>
                    </div>
                </div>
            `;
            
            // Simulate training progress
            let gbTime = 0, xgbTime = 0;
            const gbInterval = setInterval(() => {
                gbTime += 10;
                document.getElementById('gbProgress').innerHTML = `
                    <div style="width: ${gbTime}%; background: #3498db; height: 30px; border-radius: 15px; color: white; text-align: center; line-height: 30px;">
                        ${gbTime}%
                    </div>
                    <p style="margin-top: 10px;">Time: ${gbTime/10}s</p>
                `;
                if (gbTime >= 100) clearInterval(gbInterval);
            }, 100);
            
            const xgbInterval = setInterval(() => {
                xgbTime += 25;
                document.getElementById('xgbProgress').innerHTML = `
                    <div style="width: ${xgbTime}%; background: #e74c3c; height: 30px; border-radius: 15px; color: white; text-align: center; line-height: 30px;">
                        ${xgbTime}%
                    </div>
                    <p style="margin-top: 10px;">Time: ${xgbTime/25}s</p>
                    ${xgbTime >= 100 ? '<p style="color: green; font-weight: bold;">‚úÖ 2.5x Faster!</p>' : ''}
                `;
                if (xgbTime >= 100) clearInterval(xgbInterval);
            }, 100);
        }
    </script>
</body>
</html>

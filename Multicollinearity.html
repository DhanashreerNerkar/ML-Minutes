<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multicollinearity - Why It's a Problem in Linear Regression</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
        }

        .header {
            text-align: center;
            padding: 60px 30px;
            background: rgba(255, 255, 255, 0.98);
            border-radius: 30px;
            margin-bottom: 40px;
            box-shadow: 0 30px 60px rgba(0, 0, 0, 0.15);
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 5px;
            background: linear-gradient(90deg, #ef4444, #f59e0b, #ef4444);
            animation: shimmer 3s infinite;
        }

        @keyframes shimmer {
            0% { transform: translateX(-100%); }
            100% { transform: translateX(100%); }
        }

        .header h1 {
            font-size: 3em;
            background: linear-gradient(135deg, #ef4444 0%, #f59e0b 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 15px;
            animation: fadeInUp 0.8s ease;
        }

        .header p {
            font-size: 1.3em;
            color: #666;
            animation: fadeInUp 1s ease;
        }

        .card {
            background: white;
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 15px 35px rgba(0, 0, 0, 0.1);
            transition: all 0.3s ease;
            animation: fadeInUp 0.6s ease;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
        }

        .section-title {
            font-size: 2em;
            margin-bottom: 25px;
            color: #1a1a1a;
            display: flex;
            align-items: center;
            gap: 15px;
        }

        .icon-box {
            width: 50px;
            height: 50px;
            background: linear-gradient(135deg, #ef4444, #f59e0b);
            border-radius: 15px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 24px;
        }

        .visual-demo {
            background: linear-gradient(135deg, #f5f7fa 0%, #e9ecef 100%);
            border-radius: 15px;
            padding: 30px;
            margin: 25px 0;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 300px;
        }

        .concept-box {
            background: linear-gradient(135deg, #fef3c7, #fed7aa);
            border: 2px solid #f59e0b;
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
        }

        .concept-box h3 {
            color: #92400e;
            margin-bottom: 15px;
            font-size: 1.4em;
        }

        .problem-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }

        .problem-card {
            background: linear-gradient(135deg, #fee2e2, #fecaca);
            border-left: 4px solid #ef4444;
            border-radius: 10px;
            padding: 20px;
            transition: transform 0.3s ease;
        }

        .problem-card:hover {
            transform: translateX(5px);
        }

        .problem-card h4 {
            color: #991b1b;
            margin-bottom: 10px;
            font-size: 1.2em;
        }

        .solution-card {
            background: linear-gradient(135deg, #d4f4dd, #bbf7d0);
            border-left: 4px solid #22c55e;
            border-radius: 10px;
            padding: 20px;
            margin: 15px 0;
            transition: transform 0.3s ease;
        }

        .solution-card:hover {
            transform: translateX(5px);
        }

        .solution-card h4 {
            color: #166534;
            margin-bottom: 10px;
            font-size: 1.2em;
        }

        .code-block {
            background: #1e293b;
            color: #e2e8f0;
            border-radius: 15px;
            padding: 25px;
            margin: 25px 0;
            overflow-x: auto;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.95em;
            line-height: 1.8;
            position: relative;
        }

        .code-block::before {
            content: 'Python';
            position: absolute;
            top: 10px;
            right: 10px;
            background: rgba(99, 102, 241, 0.2);
            color: #818cf8;
            padding: 4px 12px;
            border-radius: 6px;
            font-size: 0.8em;
        }

        .warning-box {
            background: linear-gradient(135deg, #fef3c7, #fde68a);
            border: 2px solid #f59e0b;
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
            display: flex;
            align-items: start;
            gap: 15px;
        }

        .warning-icon {
            font-size: 30px;
            color: #f59e0b;
        }

        .example-box {
            background: linear-gradient(135deg, #e0e7ff, #c7d2fe);
            border-radius: 15px;
            padding: 25px;
            margin: 25px 0;
            border: 2px solid #6366f1;
        }

        .example-box h4 {
            color: #312e81;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }

        .comparison-table th {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }

        .comparison-table td {
            padding: 15px;
            border-bottom: 1px solid #e5e7eb;
        }

        .comparison-table tr:hover {
            background: #f8fafc;
        }

        .metric-display {
            display: flex;
            justify-content: space-around;
            margin: 20px 0;
        }

        .metric-card {
            text-align: center;
            padding: 20px;
            background: white;
            border-radius: 15px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            transition: transform 0.3s ease;
        }

        .metric-card:hover {
            transform: scale(1.05);
        }

        .metric-value {
            font-size: 2.5em;
            font-weight: bold;
            margin: 10px 0;
        }

        .metric-label {
            color: #666;
            font-size: 0.9em;
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        .good { color: #22c55e; }
        .warning { color: #f59e0b; }
        .bad { color: #ef4444; }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.05); }
        }

        .animated-correlation {
            animation: pulse 2s infinite;
        }

        @media (max-width: 768px) {
            .header h1 { font-size: 2em; }
            .problem-grid { grid-template-columns: 1fr; }
            .metric-display { flex-direction: column; gap: 20px; }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>‚ö†Ô∏è Multicollinearity</h1>
            <p>Why It's a Silent Killer in Linear Regression</p>
        </div>

        <!-- What is Multicollinearity -->
        <div class="card">
            <h2 class="section-title">
                <span class="icon-box">üîç</span>
                What is Multicollinearity?
            </h2>
            
            <p style="font-size: 1.2em; line-height: 1.8; color: #475569; margin-bottom: 20px;">
                <strong>Multicollinearity</strong> occurs when two or more independent variables in a regression model are highly correlated with each other. 
                This means they contain similar information about the variance in the dependent variable.
            </p>

            <div class="visual-demo">
                <svg width="600" height="400" viewBox="0 0 600 400">
                    <!-- Perfect Multicollinearity -->
                    <g transform="translate(0, 0)">
                        <text x="150" y="30" text-anchor="middle" font-weight="bold" font-size="16" fill="#ef4444">Perfect Multicollinearity</text>
                        <text x="150" y="50" text-anchor="middle" font-size="12" fill="#666">X‚ÇÇ = 2 * X‚ÇÅ</text>
                        
                        <!-- Scatter plot showing perfect correlation -->
                        <rect x="50" y="70" width="200" height="150" fill="#f8f9fa" stroke="#ddd"/>
                        <line x1="70" y1="190" x2="230" y2="90" stroke="#ef4444" stroke-width="3"/>
                        
                        <!-- Points on the line -->
                        <circle cx="70" cy="190" r="4" fill="#ef4444"/>
                        <circle cx="100" cy="170" r="4" fill="#ef4444"/>
                        <circle cx="130" cy="150" r="4" fill="#ef4444"/>
                        <circle cx="160" cy="130" r="4" fill="#ef4444"/>
                        <circle cx="190" cy="110" r="4" fill="#ef4444"/>
                        <circle cx="220" cy="90" r="4" fill="#ef4444"/>
                        
                        <text x="150" y="240" text-anchor="middle" font-size="14" fill="#ef4444">r = 1.00 (100% correlated)</text>
                    </g>

                    <!-- High Multicollinearity -->
                    <g transform="translate(300, 0)">
                        <text x="150" y="30" text-anchor="middle" font-weight="bold" font-size="16" fill="#f59e0b">High Multicollinearity</text>
                        <text x="150" y="50" text-anchor="middle" font-size="12" fill="#666">X‚ÇÇ ‚âà 2 * X‚ÇÅ + noise</text>
                        
                        <!-- Scatter plot showing high correlation -->
                        <rect x="50" y="70" width="200" height="150" fill="#f8f9fa" stroke="#ddd"/>
                        <line x1="70" y1="190" x2="230" y2="90" stroke="#f59e0b" stroke-width="2" stroke-dasharray="5,5"/>
                        
                        <!-- Points scattered around the line -->
                        <circle cx="75" cy="185" r="4" fill="#f59e0b"/>
                        <circle cx="95" cy="175" r="4" fill="#f59e0b"/>
                        <circle cx="135" cy="145" r="4" fill="#f59e0b"/>
                        <circle cx="155" cy="135" r="4" fill="#f59e0b"/>
                        <circle cx="185" cy="115" r="4" fill="#f59e0b"/>
                        <circle cx="215" cy="95" r="4" fill="#f59e0b"/>
                        
                        <text x="150" y="240" text-anchor="middle" font-size="14" fill="#f59e0b">r = 0.95 (95% correlated)</text>
                    </g>

                    <!-- No Multicollinearity -->
                    <g transform="translate(0, 180)">
                        <text x="150" y="30" text-anchor="middle" font-weight="bold" font-size="16" fill="#22c55e">No Multicollinearity</text>
                        <text x="150" y="50" text-anchor="middle" font-size="12" fill="#666">X‚ÇÅ and X‚ÇÇ independent</text>
                        
                        <!-- Scatter plot showing no correlation -->
                        <rect x="50" y="70" width="200" height="150" fill="#f8f9fa" stroke="#ddd"/>
                        
                        <!-- Random scatter -->
                        <circle cx="80" cy="120" r="4" fill="#22c55e"/>
                        <circle cx="200" cy="180" r="4" fill="#22c55e"/>
                        <circle cx="130" cy="100" r="4" fill="#22c55e"/>
                        <circle cx="180" cy="140" r="4" fill="#22c55e"/>
                        <circle cx="100" cy="160" r="4" fill="#22c55e"/>
                        <circle cx="220" cy="110" r="4" fill="#22c55e"/>
                        <circle cx="70" cy="150" r="4" fill="#22c55e"/>
                        <circle cx="160" cy="90" r="4" fill="#22c55e"/>
                        
                        <text x="150" y="240" text-anchor="middle" font-size="14" fill="#22c55e">r = 0.15 (15% correlated)</text>
                    </g>

                    <!-- Correlation Matrix Visualization -->
                    <g transform="translate(350, 180)">
                        <text x="100" y="30" text-anchor="middle" font-weight="bold" font-size="16" fill="#333">Correlation Matrix</text>
                        
                        <!-- Matrix cells -->
                        <rect x="50" y="60" width="40" height="40" fill="#22c55e"/>
                        <rect x="90" y="60" width="40" height="40" fill="#fbbf24"/>
                        <rect x="130" y="60" width="40" height="40" fill="#ef4444"/>
                        
                        <rect x="50" y="100" width="40" height="40" fill="#fbbf24"/>
                        <rect x="90" y="100" width="40" height="40" fill="#22c55e"/>
                        <rect x="130" y="100" width="40" height="40" fill="#fb923c"/>
                        
                        <rect x="50" y="140" width="40" height="40" fill="#ef4444"/>
                        <rect x="90" y="140" width="40" height="40" fill="#fb923c"/>
                        <rect x="130" y="140" width="40" height="40" fill="#22c55e"/>
                        
                        <!-- Labels -->
                        <text x="30" y="85" font-size="12">X‚ÇÅ</text>
                        <text x="30" y="125" font-size="12">X‚ÇÇ</text>
                        <text x="30" y="165" font-size="12">X‚ÇÉ</text>
                        
                        <text x="70" y="50" font-size="12">X‚ÇÅ</text>
                        <text x="110" y="50" font-size="12">X‚ÇÇ</text>
                        <text x="150" y="50" font-size="12">X‚ÇÉ</text>
                        
                        <!-- Legend -->
                        <rect x="50" y="200" width="15" height="15" fill="#22c55e"/>
                        <text x="70" y="212" font-size="11">Low (0-0.3)</text>
                        
                        <rect x="50" y="220" width="15" height="15" fill="#fbbf24"/>
                        <text x="70" y="232" font-size="11">Medium (0.3-0.7)</text>
                        
                        <rect x="50" y="240" width="15" height="15" fill="#ef4444"/>
                        <text x="70" y="252" font-size="11">High (0.7-1.0)</text>
                    </g>
                </svg>
            </div>

            <div class="concept-box">
                <h3>üìä Simple Analogy</h3>
                <p>Imagine you're trying to predict a person's weight using both:</p>
                <ul style="margin-left: 20px; line-height: 1.8;">
                    <li><strong>Height in inches</strong></li>
                    <li><strong>Height in centimeters</strong></li>
                </ul>
                <p style="margin-top: 15px;">
                    These two variables contain <strong>exactly the same information</strong> (perfect multicollinearity). 
                    The model can't determine which variable is actually contributing to the prediction because they move together perfectly.
                </p>
            </div>
        </div>

        <!-- Why is Multicollinearity Bad? -->
        <div class="card">
            <h2 class="section-title">
                <span class="icon-box">üí•</span>
                Why is Multicollinearity a Problem?
            </h2>

            <div class="problem-grid">
                <div class="problem-card">
                    <h4>1. üìä Unstable Coefficients</h4>
                    <p>Small changes in data lead to large changes in coefficient estimates. Your model becomes unreliable and non-reproducible.</p>
                    <div style="margin-top: 10px; padding: 10px; background: #fff; border-radius: 8px;">
                        <code>Without noise: Œ≤‚ÇÅ = 2.5</code><br>
                        <code>With 1% noise: Œ≤‚ÇÅ = -15.3 üò±</code>
                    </div>
                </div>

                <div class="problem-card">
                    <h4>2. üìà Inflated Standard Errors</h4>
                    <p>Standard errors of coefficients become very large, making confidence intervals wider and hypothesis tests unreliable.</p>
                    <div style="margin-top: 10px; padding: 10px; background: #fff; border-radius: 8px;">
                        <code>Normal SE: 0.05</code><br>
                        <code>With multicollinearity: 5.83 üò∞</code>
                    </div>
                </div>

                <div class="problem-card">
                    <h4>3. ‚ùì Wrong Signs</h4>
                    <p>Coefficients may have counterintuitive signs (negative when should be positive), misleading interpretation.</p>
                    <div style="margin-top: 10px; padding: 10px; background: #fff; border-radius: 8px;">
                        <code>Expected: Price ‚Üë as Size ‚Üë</code><br>
                        <code>Result: Negative coefficient! ü§Ø</code>
                    </div>
                </div>

                <div class="problem-card">
                    <h4>4. üéØ Poor Interpretability</h4>
                    <p>Can't determine individual feature importance or contribution. Model becomes a "black box" for feature effects.</p>
                </div>

                <div class="problem-card">
                    <h4>5. üîÑ Overfitting Risk</h4>
                    <p>Model fits training data noise rather than true patterns, leading to poor generalization on new data.</p>
                </div>

                <div class="problem-card">
                    <h4>6. üßÆ Computational Issues</h4>
                    <p>Matrix (X'X) becomes nearly singular, causing numerical instability and computation errors.</p>
                </div>
            </div>

            <div class="warning-box">
                <span class="warning-icon">‚ö†Ô∏è</span>
                <div>
                    <strong>Critical Point:</strong> Even with high multicollinearity, your model might still make good predictions! 
                    The problem is that you can't trust the individual coefficients or understand which features are actually important. 
                    It's like having a working black box - it works, but you don't know why or how.
                </div>
            </div>
        </div>

        <!-- Real Example with Code -->
        <div class="card">
            <h2 class="section-title">
                <span class="icon-box">üíª</span>
                Real Example: The Multicollinearity Disaster
            </h2>

            <div class="code-block">
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)
n = 100

# Scenario 1: No Multicollinearity
print("="*60)
print("SCENARIO 1: NO MULTICOLLINEARITY")
print("="*60)

# Independent features
X1 = np.random.randn(n)
X2 = np.random.randn(n)  # Completely independent
X_no_multi = np.column_stack([X1, X2])

# True relationship: y = 3*X1 + 2*X2 + noise
y = 3*X1 + 2*X2 + np.random.randn(n)*0.1

# Fit model
model1 = LinearRegression()
model1.fit(X_no_multi, y)

print(f"True coefficients:      [3.0, 2.0]")
print(f"Estimated coefficients: [{model1.coef_[0]:.2f}, {model1.coef_[1]:.2f}]")
print(f"R¬≤ Score: {r2_score(y, model1.predict(X_no_multi)):.4f}")
print(f"Correlation between X1 and X2: {np.corrcoef(X1, X2)[0,1]:.4f}")

# Scenario 2: High Multicollinearity
print("\n" + "="*60)
print("SCENARIO 2: HIGH MULTICOLLINEARITY")
print("="*60)

# Highly correlated features
X1 = np.random.randn(n)
X2 = X1 + np.random.randn(n)*0.1  # X2 is almost equal to X1
X_high_multi = np.column_stack([X1, X2])

# Same true relationship
y = 3*X1 + 2*X2 + np.random.randn(n)*0.1

# Fit model
model2 = LinearRegression()
model2.fit(X_high_multi, y)

print(f"True coefficients:      [3.0, 2.0]")
print(f"Estimated coefficients: [{model2.coef_[0]:.2f}, {model2.coef_[1]:.2f}]")
print(f"R¬≤ Score: {r2_score(y, model2.predict(X_high_multi)):.4f}")
print(f"Correlation between X1 and X2: {np.corrcoef(X1, X2)[0,1]:.4f}")

# Show instability with small data change
print("\n" + "="*60)
print("DEMONSTRATING INSTABILITY")
print("="*60)

# Add tiny noise to one data point
X_high_multi_noisy = X_high_multi.copy()
X_high_multi_noisy[0, 0] += 0.01  # Tiny change!

# Refit model
model3 = LinearRegression()
model3.fit(X_high_multi_noisy, y)

print(f"Original coefficients:    [{model2.coef_[0]:.2f}, {model2.coef_[1]:.2f}]")
print(f"After 0.01 change:        [{model3.coef_[0]:.2f}, {model3.coef_[1]:.2f}]")
print(f"Coefficient change:       [{abs(model3.coef_[0]-model2.coef_[0]):.2f}, {abs(model3.coef_[1]-model2.coef_[1]):.2f}]")
print("\n‚ö†Ô∏è  Tiny data change caused huge coefficient changes!")

# Calculate VIF (Variance Inflation Factor)
print("\n" + "="*60)
print("VARIANCE INFLATION FACTOR (VIF)")
print("="*60)

from sklearn.linear_model import LinearRegression

def calculate_vif(X):
    vif = []
    for i in range(X.shape[1]):
        # Get all other features
        X_others = np.delete(X, i, axis=1)
        X_i = X[:, i]
        
        # Regress X_i on other features
        reg = LinearRegression()
        reg.fit(X_others, X_i)
        r_squared = r2_score(X_i, reg.predict(X_others))
        
        # Calculate VIF
        vif_value = 1 / (1 - r_squared) if r_squared < 1 else float('inf')
        vif.append(vif_value)
    
    return vif

# Calculate VIF for both scenarios
vif_no_multi = calculate_vif(X_no_multi)
vif_high_multi = calculate_vif(X_high_multi)

print("VIF Values (>10 indicates multicollinearity):")
print(f"No Multicollinearity:   X1={vif_no_multi[0]:.2f}, X2={vif_no_multi[1]:.2f} ‚úÖ")
print(f"High Multicollinearity: X1={vif_high_multi[0]:.2f}, X2={vif_high_multi[1]:.2f} ‚ùå")

# Real-world example
print("\n" + "="*60)
print("REAL WORLD EXAMPLE: HOUSE PRICES")
print("="*60)

# Create correlated house features
n = 200
house_size_sqft = np.random.uniform(800, 3000, n)
num_rooms = house_size_sqft / 200 + np.random.randn(n) * 0.5  # Highly correlated
num_bathrooms = num_rooms / 3 + np.random.randn(n) * 0.2  # Also correlated
garage_size = house_size_sqft / 500 + np.random.randn(n) * 0.3  # Correlated

# Create feature matrix
X_house = np.column_stack([house_size_sqft, num_rooms, num_bathrooms, garage_size])

# True house price (only really depends on size)
house_price = 100 * house_size_sqft + np.random.randn(n) * 10000

# Fit model
model_house = LinearRegression()
model_house.fit(X_house, house_price)

# Check correlations
corr_matrix = np.corrcoef(X_house.T)
feature_names = ['Size(sqft)', 'Rooms', 'Bathrooms', 'Garage']

print("Correlation Matrix:")
print("              Size    Rooms   Bathrooms  Garage")
for i, name in enumerate(feature_names):
    print(f"{name:12}", end="")
    for j in range(4):
        color = "üî¥" if abs(corr_matrix[i,j]) > 0.8 and i != j else "üü¢"
        print(f"  {corr_matrix[i,j]:.2f}{color}", end="")
    print()

print("\nModel Coefficients (should all be positive for house prices!):")
for name, coef in zip(feature_names, model_house.coef_):
    sign = "‚úÖ" if coef > 0 else "‚ùå"
    print(f"{name:12}: {coef:>10.2f} {sign}")

print(f"\nR¬≤ Score: {r2_score(house_price, model_house.predict(X_house)):.4f}")
print("\n‚ö†Ô∏è  Notice: Some coefficients might be negative or huge/tiny!")
print("This doesn't mean those features decrease house prices.")
print("It's multicollinearity causing unstable coefficients!")
            </div>

            <div class="metric-display">
                <div class="metric-card">
                    <div class="metric-label">No Multicollinearity</div>
                    <div class="metric-value good">VIF &lt; 5</div>
                    <div>Safe to interpret ‚úÖ</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">Moderate</div>
                    <div class="metric-value warning">VIF 5-10</div>
                    <div>Be cautious ‚ö†Ô∏è</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">Severe</div>
                    <div class="metric-value bad">VIF &gt; 10</div>
                    <div>Don't trust coefficients ‚ùå</div>
                </div>
            </div>
        </div>

        <!-- How to Detect -->
        <div class="card">
            <h2 class="section-title">
                <span class="icon-box">üîç</span>
                How to Detect Multicollinearity
            </h2>

            <div class="solution-card">
                <h4>1. Variance Inflation Factor (VIF) - Most Common</h4>
                <p><strong>Formula:</strong> VIF = 1 / (1 - R¬≤·µ¢)</p>
                <p>Where R¬≤·µ¢ is the R¬≤ from regressing X·µ¢ on all other X variables</p>
                <ul style="margin-top: 10px; margin-left: 20px;">
                    <li>VIF = 1: No correlation</li>
                    <li>VIF &lt; 5: Low multicollinearity</li>
                    <li>VIF 5-10: Moderate (be careful)</li>
                    <li>VIF &gt; 10: High (serious problem)</li>
                </ul>
            </div>

            <div class="solution-card">
                <h4>2. Correlation Matrix</h4>
                <p>Check pairwise correlations between features</p>
                <ul style="margin-top: 10px; margin-left: 20px;">
                    <li>|r| &lt; 0.7: Generally acceptable</li>
                    <li>|r| &gt; 0.8: Potential problem</li>
                    <li>|r| &gt; 0.9: Serious multicollinearity</li>
                </ul>
            </div>

            <div class="solution-card">
                <h4>3. Condition Number</h4>
                <p>Condition number of X'X matrix</p>
                <ul style="margin-top: 10px; margin-left: 20px;">
                    <li>&lt; 10: No multicollinearity</li>
                    <li>10-30: Moderate</li>
                    <li>&gt; 30: Severe</li>
                    <li>&gt; 1000: Critical</li>
                </ul>
            </div>

            <div class="solution-card">
                <h4>4. Warning Signs in Results</h4>
                <ul style="margin-left: 20px;">
                    <li>Large standard errors despite high R¬≤</li>
                    <li>Coefficients with wrong signs</li>
                    <li>Huge coefficient magnitudes</li>
                    <li>Coefficients change dramatically with small data changes</li>
                    <li>Non-significant variables that should be significant</li>
                </ul>
            </div>
        </div>

        <!-- Solutions -->
        <div class="card">
            <h2 class="section-title">
                <span class="icon-box">üí°</span>
                How to Fix Multicollinearity
            </h2>

            <div class="solution-card">
                <h4>‚úÖ Solution 1: Remove Highly Correlated Features</h4>
                <p>Drop one of the correlated variables. Keep the one with stronger theoretical importance or better data quality.</p>
                <code style="background: #f1f5f9; padding: 10px; border-radius: 5px; display: block; margin-top: 10px;">
                    # Remove features with correlation > 0.9<br>
                    corr_matrix = df.corr().abs()<br>
                    upper_tri = corr_matrix.where(np.triu(np.ones_like(corr_matrix), k=1).astype(bool))<br>
                    to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > 0.9)]<br>
                    df = df.drop(columns=to_drop)
                </code>
            </div>

            <div class="solution-card">
                <h4>‚úÖ Solution 2: Ridge Regression (L2 Regularization)</h4>
                <p>Add penalty term to shrink coefficients, reducing impact of multicollinearity.</p>
                <code style="background: #f1f5f9; padding: 10px; border-radius: 5px; display: block; margin-top: 10px;">
                    from sklearn.linear_model import Ridge<br>
                    model = Ridge(alpha=1.0)  # Handles multicollinearity well
                </code>
            </div>

            <div class="solution-card">
                <h4>‚úÖ Solution 3: Principal Component Analysis (PCA)</h4>
                <p>Transform correlated features into uncorrelated principal components.</p>
                <code style="background: #f1f5f9; padding: 10px; border-radius: 5px; display: block; margin-top: 10px;">
                    from sklearn.decomposition import PCA<br>
                    pca = PCA(n_components=0.95)  # Keep 95% variance<br>
                    X_pca = pca.fit_transform(X)
                </code>
            </div>

            <div class="solution-card">
                <h4>‚úÖ Solution 4: Combine Features</h4>
                <p>Create composite variables from correlated features.</p>
                <ul style="margin-top: 10px; margin-left: 20px;">
                    <li>Instead of height_inches and height_cm, use just one</li>
                    <li>Combine revenue_q1, revenue_q2, etc. into annual_revenue</li>
                    <li>Average multiple similar measurements</li>
                </ul>
            </div>

            <div class="solution-card">
                <h4>‚úÖ Solution 5: Increase Sample Size</h4>
                <p>More data can help stabilize coefficient estimates (though doesn't eliminate the problem).</p>
            </div>

            <div class="solution-card">
                <h4>‚úÖ Solution 6: Centering and Scaling</h4>
                <p>Standardize variables to reduce numerical issues.</p>
                <code style="background: #f1f5f9; padding: 10px; border-radius: 5px; display: block; margin-top: 10px;">
                    from sklearn.preprocessing import StandardScaler<br>
                    scaler = StandardScaler()<br>
                    X_scaled = scaler.fit_transform(X)
                </code>
            </div>
        </div>

        <!-- When it Matters -->
        <div class="card">
            <h2 class="section-title">
                <span class="icon-box">üéØ</span>
                When Does Multicollinearity Matter?
            </h2>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>Multicollinearity Impact</th>
                        <th>Should You Worry?</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Prediction Only</strong></td>
                        <td>Predictions may still be accurate</td>
                        <td><span class="good">Less Critical ‚úÖ</span></td>
                    </tr>
                    <tr>
                        <td><strong>Feature Importance</strong></td>
                        <td>Can't trust individual coefficients</td>
                        <td><span class="bad">Very Critical ‚ùå</span></td>
                    </tr>
                    <tr>
                        <td><strong>Causal Inference</strong></td>
                        <td>Can't determine true relationships</td>
                        <td><span class="bad">Very Critical ‚ùå</span></td>
                    </tr>
                    <tr>
                        <td><strong>Variable Selection</strong></td>
                        <td>May select wrong variables</td>
                        <td><span class="bad">Critical ‚ùå</span></td>
                    </tr>
                    <tr>
                        <td><strong>Hypothesis Testing</strong></td>
                        <td>P-values unreliable</td>
                        <td><span class="bad">Critical ‚ùå</span></td>
                    </tr>
                    <tr>
                        <td><strong>Forecasting (in-sample)</strong></td>
                        <td>May work if patterns continue</td>
                        <td><span class="warning">Moderate ‚ö†Ô∏è</span></td>
                    </tr>
                </tbody>
            </table>
        </div>

        <!-- Real World Examples -->
        <div class="card">
            <h2 class="section-title">
                <span class="icon-box">üåç</span>
                Real-World Examples
            </h2>

            <div class="example-box">
                <h4>üè¢ Example 1: Marketing Campaign Analysis</h4>
                <p><strong>Problem:</strong> Company runs TV, Radio, and Online ads simultaneously</p>
                <p><strong>Multicollinearity:</strong> All ad spending increases together during peak seasons</p>
                <p><strong>Consequence:</strong> Can't determine which channel is actually driving sales</p>
                <p><strong>Solution:</strong> Run controlled experiments with varying spend levels</p>
            </div>

            <div class="example-box">
                <h4>üìà Example 2: Stock Market Prediction</h4>
                <p><strong>Problem:</strong> Using P/E ratio, Price/Book ratio, and Market Cap</p>
                <p><strong>Multicollinearity:</strong> All ratios include stock price</p>
                <p><strong>Consequence:</strong> Model is essentially using price to predict price</p>
                <p><strong>Solution:</strong> Use fundamental metrics instead of ratios</p>
            </div>

            <div class="example-box">
                <h4>üè• Example 3: Medical Research</h4>
                <p><strong>Problem:</strong> Predicting heart disease using age, years smoking, pack-years</p>
                <p><strong>Multicollinearity:</strong> Pack-years = years smoking √ó packs per day</p>
                <p><strong>Consequence:</strong> Can't separate effect of duration vs intensity</p>
                <p><strong>Solution:</strong> Choose either pack-years OR separate variables, not both</p>
            </div>
        </div>

        <!-- Summary -->
        <div class="card">
            <h2 class="section-title">
                <span class="icon-box">üìã</span>
                Key Takeaways
            </h2>

            <div style="background: linear-gradient(135deg, #f0f9ff, #e0f2fe); padding: 30px; border-radius: 15px; border: 2px solid #3b82f6;">
                <h3 style="color: #1e40af; margin-bottom: 20px;">Remember:</h3>
                <ol style="line-height: 2; font-size: 1.1em;">
                    <li><strong>Multicollinearity doesn't affect prediction accuracy</strong>, but makes coefficients unreliable</li>
                    <li><strong>Always check VIF</strong> before interpreting coefficients</li>
                    <li><strong>High R¬≤ with non-significant variables</strong> is a red flag</li>
                    <li><strong>Ridge/Lasso regression</strong> are good default choices when multicollinearity exists</li>
                    <li><strong>Domain knowledge</strong> is crucial for deciding which variables to keep</li>
                    <li><strong>Perfect multicollinearity</strong> (exact linear relationship) will cause computational errors</li>
                </ol>
            </div>

            <div class="warning-box" style="margin-top: 20px;">
                <span class="warning-icon">üí°</span>
                <div>
                    <strong>Golden Rule:</strong> If you care about understanding which features matter and by how much, 
                    multicollinearity is your enemy. If you only care about predictions, it might not matter as much.
                </div>
            </div>
        </div>

    </div>
</body>
</html>

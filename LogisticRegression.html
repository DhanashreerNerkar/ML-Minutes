<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Logistic Regression - Complete Visual Guide with Real Examples</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
        }

        .header {
            text-align: center;
            padding: 60px 30px;
            background: rgba(255, 255, 255, 0.98);
            border-radius: 30px;
            margin-bottom: 40px;
            box-shadow: 0 30px 60px rgba(0, 0, 0, 0.15);
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 5px;
            background: linear-gradient(90deg, #10b981, #3b82f6, #8b5cf6, #ec4899);
            animation: shimmer 3s infinite;
        }

        @keyframes shimmer {
            0% { transform: translateX(-100%); }
            100% { transform: translateX(100%); }
        }

        .header h1 {
            font-size: 3.5em;
            background: linear-gradient(135deg, #3b82f6 0%, #8b5cf6 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 15px;
            animation: fadeInUp 0.8s ease;
        }

        .header p {
            font-size: 1.3em;
            color: #666;
            animation: fadeInUp 1s ease;
        }

        .nav-tabs {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin-bottom: 40px;
            justify-content: center;
            animation: fadeIn 1.2s ease;
        }

        .nav-tab {
            padding: 15px 30px;
            background: white;
            border: none;
            border-radius: 50px;
            cursor: pointer;
            font-size: 1.1em;
            font-weight: 600;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        .nav-tab:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 15px rgba(0, 0, 0, 0.2);
        }

        .nav-tab.active {
            background: linear-gradient(135deg, #3b82f6 0%, #8b5cf6 100%);
            color: white;
        }

        .content-section {
            display: none;
            animation: fadeInUp 0.6s ease;
        }

        .content-section.active {
            display: block;
        }

        .card {
            background: white;
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 15px 35px rgba(0, 0, 0, 0.1);
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }

        .card::before {
            content: '';
            position: absolute;
            top: 0;
            left: -100%;
            width: 100%;
            height: 100%;
            background: linear-gradient(90deg, transparent, rgba(255,255,255,0.3), transparent);
            transition: left 0.5s;
        }

        .card:hover::before {
            left: 100%;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
        }

        .section-title {
            font-size: 2.2em;
            margin-bottom: 25px;
            color: #1a1a1a;
            display: flex;
            align-items: center;
            gap: 15px;
        }

        .icon-box {
            width: 50px;
            height: 50px;
            background: linear-gradient(135deg, #3b82f6, #8b5cf6);
            border-radius: 15px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 24px;
        }

        .visual-demo {
            background: linear-gradient(135deg, #f5f7fa 0%, #e9ecef 100%);
            border-radius: 15px;
            padding: 30px;
            margin: 25px 0;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 300px;
        }

        .formula-box {
            background: linear-gradient(135deg, #3b82f6 0%, #8b5cf6 100%);
            color: white;
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
            text-align: center;
            font-size: 1.3em;
            font-weight: 600;
            box-shadow: 0 10px 25px rgba(59, 130, 246, 0.3);
            animation: pulse 2s infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.02); }
        }

        .code-block {
            background: #1e293b;
            color: #e2e8f0;
            border-radius: 15px;
            padding: 25px;
            margin: 25px 0;
            overflow-x: auto;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.95em;
            line-height: 1.8;
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.2);
            position: relative;
        }

        .code-block::before {
            content: 'Python';
            position: absolute;
            top: 10px;
            right: 10px;
            background: rgba(99, 102, 241, 0.2);
            color: #818cf8;
            padding: 4px 12px;
            border-radius: 6px;
            font-size: 0.8em;
        }

        .concept-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }

        .concept-card {
            background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%);
            border-left: 4px solid #3b82f6;
            border-radius: 10px;
            padding: 20px;
            transition: transform 0.3s ease;
        }

        .concept-card:hover {
            transform: translateX(5px);
        }

        .concept-card h4 {
            color: #1e40af;
            margin-bottom: 10px;
            font-size: 1.2em;
        }

        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 25px 0;
        }

        .pros, .cons {
            padding: 25px;
            border-radius: 15px;
        }

        .pros {
            background: linear-gradient(135deg, #d4f4dd 0%, #bbf7d0 100%);
            border: 2px solid #22c55e;
        }

        .cons {
            background: linear-gradient(135deg, #fee2e2 0%, #fecaca 100%);
            border: 2px solid #ef4444;
        }

        .pros h4, .cons h4 {
            margin-bottom: 15px;
            font-size: 1.3em;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .interview-section {
            background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%);
            border-radius: 15px;
            padding: 30px;
            margin: 25px 0;
            border: 2px solid #f59e0b;
        }

        .interview-section h4 {
            color: #92400e;
            margin-bottom: 20px;
            font-size: 1.4em;
        }

        .question-item {
            background: white;
            border-radius: 10px;
            padding: 15px;
            margin-bottom: 15px;
            border-left: 4px solid #f59e0b;
            transition: transform 0.3s ease;
        }

        .question-item:hover {
            transform: translateX(5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .real-example {
            background: linear-gradient(135deg, #e0e7ff 0%, #c7d2fe 100%);
            border-radius: 15px;
            padding: 25px;
            margin: 25px 0;
            border: 2px solid #6366f1;
        }

        .real-example h4 {
            color: #312e81;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }

        .comparison-table th {
            background: linear-gradient(135deg, #3b82f6, #8b5cf6);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }

        .comparison-table td {
            padding: 15px;
            border-bottom: 1px solid #e5e7eb;
        }

        .metric-display {
            display: flex;
            justify-content: space-around;
            margin: 20px 0;
            flex-wrap: wrap;
            gap: 20px;
        }

        .metric-card {
            text-align: center;
            padding: 20px;
            background: white;
            border-radius: 15px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            min-width: 150px;
            transition: transform 0.3s ease;
        }

        .metric-card:hover {
            transform: scale(1.05);
        }

        .metric-value {
            font-size: 2.5em;
            font-weight: bold;
            margin: 10px 0;
        }

        .metric-label {
            color: #666;
            font-size: 0.9em;
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        .good { color: #22c55e; }
        .warning { color: #f59e0b; }
        .bad { color: #ef4444; }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        @media (max-width: 768px) {
            .header h1 { font-size: 2.5em; }
            .pros-cons { grid-template-columns: 1fr; }
            .concept-grid { grid-template-columns: 1fr; }
        }
    </style>
    <script>
        function showSection(event, sectionId) {
            // Hide all sections
            const sections = document.querySelectorAll('.content-section');
            sections.forEach(section => {
                section.classList.remove('active');
            });
            
            // Show selected section
            document.getElementById(sectionId).classList.add('active');
            
            // Update tab styling
            const tabs = document.querySelectorAll('.nav-tab');
            tabs.forEach(tab => {
                tab.classList.remove('active');
            });
            event.target.classList.add('active');
        }

        // Interactive sigmoid visualization
        function drawSigmoid() {
            const canvas = document.getElementById('sigmoidCanvas');
            if (!canvas) return;
            
            const ctx = canvas.getContext('2d');
            const width = canvas.width;
            const height = canvas.height;
            
            // Clear canvas
            ctx.clearRect(0, 0, width, height);
            
            // Draw axes
            ctx.strokeStyle = '#666';
            ctx.lineWidth = 2;
            ctx.beginPath();
            ctx.moveTo(50, height/2);
            ctx.lineTo(width-20, height/2);
            ctx.moveTo(width/2, 20);
            ctx.lineTo(width/2, height-20);
            ctx.stroke();
            
            // Draw sigmoid curve
            ctx.strokeStyle = '#3b82f6';
            ctx.lineWidth = 3;
            ctx.beginPath();
            
            for (let x = -6; x <= 6; x += 0.1) {
                const y = 1 / (1 + Math.exp(-x));
                const canvasX = (x + 6) * (width - 70) / 12 + 50;
                const canvasY = height - 20 - y * (height - 40);
                
                if (x === -6) {
                    ctx.moveTo(canvasX, canvasY);
                } else {
                    ctx.lineTo(canvasX, canvasY);
                }
            }
            ctx.stroke();
            
            // Draw threshold line
            ctx.strokeStyle = '#ef4444';
            ctx.lineWidth = 2;
            ctx.setLineDash([5, 5]);
            ctx.beginPath();
            ctx.moveTo(50, height/2);
            ctx.lineTo(width-20, height/2);
            ctx.stroke();
            ctx.setLineDash([]);
            
            // Labels
            ctx.fillStyle = '#333';
            ctx.font = '14px sans-serif';
            ctx.fillText('0', width/2 - 15, height/2 + 20);
            ctx.fillText('1', width/2 - 15, 35);
            ctx.fillText('0.5', width/2 - 25, height/2 + 5);
            ctx.fillText('z', width - 15, height/2 + 20);
            ctx.fillText('σ(z)', width/2 + 10, 25);
        }

        window.onload = function() {
            drawSigmoid();
        };
    </script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>📈 Logistic Regression Mastery</h1>
            <p>From Binary Classification to Multiclass - Complete Visual Guide</p>
        </div>

        <div class="nav-tabs">
            <button class="nav-tab active" onclick="showSection(event, 'fundamentals')">🎯 Fundamentals</button>
            <button class="nav-tab" onclick="showSection(event, 'mathematics')">📐 Mathematics</button>
            <button class="nav-tab" onclick="showSection(event, 'implementation')">💻 Implementation</button>
            <button class="nav-tab" onclick="showSection(event, 'evaluation')">📊 Evaluation</button>
            <button class="nav-tab" onclick="showSection(event, 'realworld')">🌍 Real World</button>
            <button class="nav-tab" onclick="showSection(event, 'interview')">🎤 Interview</button>
        </div>

        <!-- Fundamentals Section -->
        <div id="fundamentals" class="content-section active">
            
            <div class="card">
                <h2 class="section-title">
                    <span class="icon-box">📊</span>
                    What is Logistic Regression?
                </h2>
                
                <p style="font-size: 1.2em; line-height: 1.8; color: #475569; margin-bottom: 20px;">
                    Logistic Regression is a statistical method for predicting binary outcomes (yes/no, true/false, 0/1) 
                    by modeling the probability that an instance belongs to a particular class. Despite its name, 
                    it's a <strong>classification</strong> algorithm, not regression!
                </p>

                <div class="visual-demo">
                    <svg width="600" height="400" viewBox="0 0 600 400">
                        <!-- Linear vs Logistic Comparison -->
                        <g id="linear-regression">
                            <text x="150" y="30" text-anchor="middle" font-weight="bold" font-size="16" fill="#666">Linear Regression</text>
                            <rect x="50" y="50" width="200" height="150" fill="#f8f9fa" stroke="#ddd"/>
                            
                            <!-- Data points -->
                            <circle cx="70" cy="180" r="5" fill="#3b82f6"/>
                            <circle cx="90" cy="160" r="5" fill="#3b82f6"/>
                            <circle cx="110" cy="140" r="5" fill="#3b82f6"/>
                            <circle cx="130" cy="120" r="5" fill="#3b82f6"/>
                            <circle cx="150" cy="100" r="5" fill="#3b82f6"/>
                            <circle cx="170" cy="80" r="5" fill="#3b82f6"/>
                            <circle cx="190" cy="60" r="5" fill="#3b82f6"/>
                            
                            <!-- Regression line -->
                            <line x1="60" y1="190" x2="240" y2="10" stroke="#ef4444" stroke-width="3"/>
                            
                            <text x="150" y="220" text-anchor="middle" fill="#666" font-size="12">Unbounded Output</text>
                            <text x="150" y="235" text-anchor="middle" fill="#ef4444" font-size="11">y ∈ (-∞, +∞)</text>
                        </g>
                        
                        <g id="logistic-regression" transform="translate(300, 0)">
                            <text x="150" y="30" text-anchor="middle" font-weight="bold" font-size="16" fill="#666">Logistic Regression</text>
                            <rect x="50" y="50" width="200" height="150" fill="#f8f9fa" stroke="#ddd"/>
                            
                            <!-- Data points for two classes -->
                            <circle cx="70" cy="180" r="5" fill="#ef4444"/>
                            <circle cx="90" cy="175" r="5" fill="#ef4444"/>
                            <circle cx="110" cy="170" r="5" fill="#ef4444"/>
                            <circle cx="150" cy="125" r="5" fill="#666" stroke="#666" stroke-width="2"/>
                            <circle cx="170" cy="80" r="5" fill="#22c55e"/>
                            <circle cx="190" cy="75" r="5" fill="#22c55e"/>
                            <circle cx="210" cy="70" r="5" fill="#22c55e"/>
                            
                            <!-- Sigmoid curve -->
                            <path d="M 60 180 Q 130 180, 150 125 T 240 70" 
                                  fill="none" stroke="#8b5cf6" stroke-width="3"/>
                            
                            <!-- Decision boundary -->
                            <line x1="150" y1="50" x2="150" y2="200" stroke="#f59e0b" stroke-width="2" stroke-dasharray="5,5"/>
                            
                            <text x="150" y="220" text-anchor="middle" fill="#666" font-size="12">Probability Output</text>
                            <text x="150" y="235" text-anchor="middle" fill="#8b5cf6" font-size="11">P(y=1) ∈ [0, 1]</text>
                        </g>
                        
                        <!-- Legend -->
                        <g transform="translate(100, 280)">
                            <circle cx="10" cy="10" r="5" fill="#ef4444"/>
                            <text x="20" y="14" font-size="12" fill="#666">Class 0</text>
                            
                            <circle cx="100" cy="10" r="5" fill="#22c55e"/>
                            <text x="110" y="14" font-size="12" fill="#666">Class 1</text>
                            
                            <circle cx="200" cy="10" r="5" fill="#666" stroke="#666" stroke-width="2"/>
                            <text x="210" y="14" font-size="12" fill="#666">Decision Boundary</text>
                            
                            <line x1="320" y1="10" x2="350" y2="10" stroke="#8b5cf6" stroke-width="3"/>
                            <text x="360" y="14" font-size="12" fill="#666">Sigmoid Function</text>
                        </g>
                    </svg>
                </div>

                <div class="formula-box">
                    P(y=1|x) = σ(z) = 1 / (1 + e^(-z))
                    <br>
                    where z = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ
                </div>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>🎯 Purpose</h4>
                        <p>Predict the probability of binary outcomes (0 or 1, Yes or No, True or False)</p>
                    </div>
                    <div class="concept-card">
                        <h4>📊 Output Range</h4>
                        <p>Always between 0 and 1 (probability), unlike linear regression (-∞ to +∞)</p>
                    </div>
                    <div class="concept-card">
                        <h4>🔄 Transform Function</h4>
                        <p>Uses sigmoid/logistic function to map linear combination to probabilities</p>
                    </div>
                    <div class="concept-card">
                        <h4>🎭 Decision Boundary</h4>
                        <p>Creates a linear boundary in feature space (non-linear in probability space)</p>
                    </div>
                    <div class="concept-card">
                        <h4>⚖️ Threshold</h4>
                        <p>Default 0.5, but can be adjusted based on business needs (precision vs recall)</p>
                    </div>
                    <div class="concept-card">
                        <h4>🔍 Interpretability</h4>
                        <p>Coefficients represent log-odds change for unit increase in feature</p>
                    </div>
                </div>
            </div>

            <div class="card">
                <h2 class="section-title">
                    <span class="icon-box">🎨</span>
                    The Sigmoid Function
                </h2>

                <div class="visual-demo">
                    <canvas id="sigmoidCanvas" width="500" height="300"></canvas>
                </div>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>📈 Properties</h4>
                        <ul style="margin-left: 20px; line-height: 1.8;">
                            <li>S-shaped curve</li>
                            <li>Output always between 0 and 1</li>
                            <li>σ(0) = 0.5 (midpoint)</li>
                            <li>Differentiable everywhere</li>
                            <li>σ'(z) = σ(z)(1 - σ(z))</li>
                        </ul>
                    </div>
                    <div class="concept-card">
                        <h4>🔢 Interpretation</h4>
                        <ul style="margin-left: 20px; line-height: 1.8;">
                            <li>z > 0 → P > 0.5 → Predict Class 1</li>
                            <li>z < 0 → P < 0.5 → Predict Class 0</li>
                            <li>z = 0 → P = 0.5 → Decision boundary</li>
                            <li>|z| large → High confidence</li>
                            <li>|z| small → Low confidence</li>
                        </ul>
                    </div>
                </div>

                <div class="pros-cons">
                    <div class="pros">
                        <h4>✅ Advantages</h4>
                        <ul style="margin-left: 20px; line-height: 1.8;">
                            <li>Probabilistic predictions</li>
                            <li>No tuning required</li>
                            <li>Fast training and prediction</li>
                            <li>Works well for linearly separable data</li>
                            <li>Interpretable coefficients</li>
                            <li>Less prone to overfitting (with regularization)</li>
                            <li>Can be extended to multiclass</li>
                        </ul>
                    </div>
                    <div class="cons">
                        <h4>❌ Limitations</h4>
                        <ul style="margin-left: 20px; line-height: 1.8;">
                            <li>Assumes linear decision boundary</li>
                            <li>Sensitive to outliers</li>
                            <li>Requires more data for stable results</li>
                            <li>Can't solve non-linear problems directly</li>
                            <li>Assumes independence of features</li>
                            <li>Sensitive to multicollinearity</li>
                        </ul>
                    </div>
                </div>
            </div>

        </div>

        <!-- Mathematics Section -->
        <div id="mathematics" class="content-section">
            
            <div class="card">
                <h2 class="section-title">
                    <span class="icon-box">🧮</span>
                    Mathematical Foundation
                </h2>

                <div class="formula-box">
                    <h4>Odds and Log-Odds</h4>
                    <p style="margin: 15px 0;">
                        Odds = P(success) / P(failure) = p / (1-p)
                        <br><br>
                        Log-Odds (Logit) = ln(odds) = ln(p/(1-p)) = β₀ + β₁x₁ + ... + βₙxₙ
                    </p>
                </div>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>📊 From Linear to Logistic</h4>
                        <p><strong>Linear Model:</strong> y = β₀ + β₁x</p>
                        <p><strong>Apply Sigmoid:</strong> P(y=1) = 1/(1 + e^(-(β₀ + β₁x)))</p>
                        <p><strong>Result:</strong> Probabilities between 0 and 1</p>
                    </div>
                    <div class="concept-card">
                        <h4>🎯 Maximum Likelihood Estimation</h4>
                        <p>Find β that maximizes:</p>
                        <p><strong>L(β) = ∏ P(yᵢ|xᵢ,β)</strong></p>
                        <p>No closed-form solution → Use gradient descent</p>
                    </div>
                    <div class="concept-card">
                        <h4>📉 Cost Function (Log Loss)</h4>
                        <p><strong>J(β) = -1/m Σ[y·log(ŷ) + (1-y)·log(1-ŷ)]</strong></p>
                        <p>Convex function → Global minimum exists</p>
                    </div>
                </div>

                <div class="code-block">
# Logistic Regression Mathematics Implementation
import numpy as np
import matplotlib.pyplot as plt

class LogisticRegressionMath:
    """
    Logistic Regression from scratch with mathematical details
    """
    
    def __init__(self, learning_rate=0.01, iterations=1000):
        self.lr = learning_rate
        self.iterations = iterations
        self.costs = []
        
    def sigmoid(self, z):
        """Sigmoid activation function"""
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip to prevent overflow
    
    def sigmoid_derivative(self, z):
        """Derivative of sigmoid function"""
        s = self.sigmoid(z)
        return s * (1 - s)
    
    def cost_function(self, X, y, theta):
        """
        Binary cross-entropy loss / Log loss
        J(θ) = -1/m * Σ[y*log(h(x)) + (1-y)*log(1-h(x))]
        """
        m = len(y)
        z = X @ theta
        h = self.sigmoid(z)
        
        # Add small epsilon to prevent log(0)
        epsilon = 1e-7
        h = np.clip(h, epsilon, 1 - epsilon)
        
        cost = -1/m * (y @ np.log(h) + (1-y) @ np.log(1-h))
        return cost
    
    def gradient(self, X, y, theta):
        """
        Gradient of cost function
        ∂J/∂θ = 1/m * X^T * (h(x) - y)
        """
        m = len(y)
        z = X @ theta
        h = self.sigmoid(z)
        gradient = 1/m * X.T @ (h - y)
        return gradient
    
    def fit(self, X, y):
        """Train using gradient descent"""
        # Add bias term
        X = np.column_stack([np.ones(len(X)), X])
        m, n = X.shape
        
        # Initialize parameters
        self.theta = np.zeros(n)
        
        # Gradient descent
        for i in range(self.iterations):
            # Calculate cost
            cost = self.cost_function(X, y, self.theta)
            self.costs.append(cost)
            
            # Calculate gradient
            grad = self.gradient(X, y, self.theta)
            
            # Update parameters
            self.theta -= self.lr * grad
            
            # Print progress
            if i % 100 == 0:
                print(f"Iteration {i}, Cost: {cost:.4f}")
        
        return self
    
    def predict_proba(self, X):
        """Predict probabilities"""
        X = np.column_stack([np.ones(len(X)), X])
        return self.sigmoid(X @ self.theta)
    
    def predict(self, X, threshold=0.5):
        """Predict classes"""
        return (self.predict_proba(X) >= threshold).astype(int)
    
    def decision_boundary(self, X):
        """
        Calculate decision boundary
        For 2D: x2 = -(θ0 + θ1*x1) / θ2
        """
        if len(self.theta) == 3:  # 2D case
            x1 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
            x2 = -(self.theta[0] + self.theta[1] * x1) / self.theta[2]
            return x1, x2
        else:
            raise ValueError("Decision boundary only for 2D features")

# Demonstration
np.random.seed(42)

# Generate synthetic data
n_samples = 200
X_class0 = np.random.randn(n_samples//2, 2) + [-2, -2]
X_class1 = np.random.randn(n_samples//2, 2) + [2, 2]
X = np.vstack([X_class0, X_class1])
y = np.hstack([np.zeros(n_samples//2), np.ones(n_samples//2)])

# Train model
model = LogisticRegressionMath(learning_rate=0.1, iterations=1000)
model.fit(X, y)

# Visualize results
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# 1. Data and Decision Boundary
axes[0].scatter(X_class0[:, 0], X_class0[:, 1], c='red', label='Class 0', alpha=0.6)
axes[0].scatter(X_class1[:, 0], X_class1[:, 1], c='blue', label='Class 1', alpha=0.6)

# Plot decision boundary
x1_boundary, x2_boundary = model.decision_boundary(X)
axes[0].plot(x1_boundary, x2_boundary, 'g--', linewidth=2, label='Decision Boundary')
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')
axes[0].set_title('Logistic Regression Decision Boundary')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# 2. Cost Function Over Iterations
axes[1].plot(model.costs)
axes[1].set_xlabel('Iteration')
axes[1].set_ylabel('Cost (Log Loss)')
axes[1].set_title('Cost Function Convergence')
axes[1].grid(True, alpha=0.3)

# 3. Sigmoid Function
z = np.linspace(-10, 10, 100)
sigmoid_values = model.sigmoid(z)
axes[2].plot(z, sigmoid_values, linewidth=2, color='purple')
axes[2].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)
axes[2].axvline(x=0, color='gray', linestyle='--', alpha=0.5)
axes[2].set_xlabel('z (linear combination)')
axes[2].set_ylabel('σ(z) (probability)')
axes[2].set_title('Sigmoid Activation Function')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\nModel Parameters:")
print(f"Intercept (β₀): {model.theta[0]:.4f}")
print(f"Coefficients (β₁, β₂): {model.theta[1:]}")
print(f"Final Cost: {model.costs[-1]:.4f}")
                </div>

                <div class="formula-box">
                    <h4>Gradient Descent Update Rule</h4>
                    <p style="margin: 15px 0;">
                        θⱼ := θⱼ - α · (1/m) · Σ(h(xⁱ) - yⁱ) · xⱼⁱ
                        <br><br>
                        where h(x) = σ(θᵀx) is the hypothesis function
                    </p>
                </div>
            </div>

            <div class="card">
                <h2 class="section-title">
                    <span class="icon-box">🔄</span>
                    Regularization in Logistic Regression
                </h2>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>L2 Regularization (Ridge)</h4>
                        <p><strong>Cost:</strong> J(θ) + λΣθⱼ²</p>
                        <p><strong>Effect:</strong> Shrinks coefficients</p>
                        <p><strong>Use:</strong> Multicollinearity, overfitting</p>
                    </div>
                    <div class="concept-card">
                        <h4>L1 Regularization (Lasso)</h4>
                        <p><strong>Cost:</strong> J(θ) + λΣ|θⱼ|</p>
                        <p><strong>Effect:</strong> Feature selection</p>
                        <p><strong>Use:</strong> Sparse features</p>
                    </div>
                    <div class="concept-card">
                        <h4>Elastic Net</h4>
                        <p><strong>Cost:</strong> J(θ) + λ₁Σ|θⱼ| + λ₂Σθⱼ²</p>
                        <p><strong>Effect:</strong> Balance of L1 and L2</p>
                        <p><strong>Use:</strong> Many correlated features</p>
                    </div>
                </div>
            </div>

        </div>

        <!-- Implementation Section -->
        <div id="implementation" class="content-section">
            
            <div class="card">
                <h2 class="section-title">
                    <span class="icon-box">💻</span>
                    Complete Implementation
                </h2>

                <div class="code-block">
# Comprehensive Logistic Regression Implementation
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, confusion_matrix, 
                           classification_report, roc_curve)
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# 1. BINARY CLASSIFICATION EXAMPLE
print("="*60)
print("BINARY CLASSIFICATION - CUSTOMER CHURN PREDICTION")
print("="*60)

# Generate synthetic customer churn data
np.random.seed(42)
n_customers = 1000

# Features
tenure = np.random.uniform(0, 72, n_customers)  # Months with company
monthly_charges = np.random.uniform(20, 120, n_customers)
total_charges = tenure * monthly_charges + np.random.normal(0, 100, n_customers)
num_services = np.random.poisson(3, n_customers)
satisfaction = np.random.uniform(1, 5, n_customers)

# Create churn based on features (with some noise)
churn_prob = 1 / (1 + np.exp(
    -(-3 + 
      -0.05 * tenure + 
      0.02 * monthly_charges + 
      -0.0001 * total_charges +
      -0.3 * num_services +
      -0.5 * satisfaction +
      np.random.normal(0, 0.5, n_customers))
))
churn = (churn_prob > 0.5).astype(int)

# Create DataFrame
df = pd.DataFrame({
    'tenure': tenure,
    'monthly_charges': monthly_charges,
    'total_charges': total_charges,
    'num_services': num_services,
    'satisfaction': satisfaction,
    'churn': churn
})

print("Dataset Info:")
print(df.info())
print(f"\nChurn Rate: {df['churn'].mean():.2%}")

# Prepare data
X = df.drop('churn', axis=1)
y = df['churn']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Logistic Regression
log_reg = LogisticRegression(random_state=42, max_iter=1000)
log_reg.fit(X_train_scaled, y_train)

# Predictions
y_pred = log_reg.predict(X_test_scaled)
y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]

# Evaluation
print("\n" + "="*60)
print("MODEL PERFORMANCE")
print("="*60)

print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision: {precision_score(y_test, y_pred):.4f}")
print(f"Recall: {recall_score(y_test, y_pred):.4f}")
print(f"F1-Score: {f1_score(y_test, y_pred):.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred, 
                          target_names=['No Churn', 'Churn']))

# Feature Importance
print("\n" + "="*60)
print("FEATURE IMPORTANCE (COEFFICIENTS)")
print("="*60)

feature_importance = pd.DataFrame({
    'feature': X.columns,
    'coefficient': log_reg.coef_[0],
    'abs_coefficient': np.abs(log_reg.coef_[0])
}).sort_values('abs_coefficient', ascending=False)

print(feature_importance)

# Odds Ratios
feature_importance['odds_ratio'] = np.exp(feature_importance['coefficient'])
print("\nOdds Ratios (e^coefficient):")
print(feature_importance[['feature', 'odds_ratio']])

# 2. REGULARIZATION COMPARISON
print("\n" + "="*60)
print("REGULARIZATION COMPARISON")
print("="*60)

# Different regularization strengths
C_values = [0.001, 0.01, 0.1, 1, 10, 100]
results = []

for C in C_values:
    for penalty in ['l1', 'l2']:
        lr = LogisticRegression(
            C=C, 
            penalty=penalty, 
            solver='liblinear',
            random_state=42,
            max_iter=1000
        )
        lr.fit(X_train_scaled, y_train)
        
        # Cross-validation score
        cv_scores = cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring='roc_auc')
        
        results.append({
            'C': C,
            'penalty': penalty,
            'cv_auc_mean': cv_scores.mean(),
            'cv_auc_std': cv_scores.std(),
            'n_non_zero_coef': np.sum(lr.coef_[0] != 0)
        })

results_df = pd.DataFrame(results)
print(results_df)

# Best regularization
best_model = results_df.loc[results_df['cv_auc_mean'].idxmax()]
print(f"\nBest Model: C={best_model['C']}, Penalty={best_model['penalty']}")
print(f"CV AUC: {best_model['cv_auc_mean']:.4f} (+/- {best_model['cv_auc_std']:.4f})")

# 3. MULTICLASS CLASSIFICATION
print("\n" + "="*60)
print("MULTICLASS CLASSIFICATION")
print("="*60)

# Generate multiclass data (3 classes)
from sklearn.datasets import make_classification

X_multi, y_multi = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    n_classes=3,
    random_state=42
)

X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(
    X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi
)

# Scale
scaler_m = StandardScaler()
X_train_m_scaled = scaler_m.fit_transform(X_train_m)
X_test_m_scaled = scaler_m.transform(X_test_m)

# Multiclass strategies
strategies = {
    'ovr': 'One-vs-Rest',
    'multinomial': 'Multinomial (Softmax)'
}

for strategy, name in strategies.items():
    lr_multi = LogisticRegression(
        multi_class=strategy,
        solver='lbfgs',
        random_state=42,
        max_iter=1000
    )
    lr_multi.fit(X_train_m_scaled, y_train_m)
    
    y_pred_m = lr_multi.predict(X_test_m_scaled)
    accuracy = accuracy_score(y_test_m, y_pred_m)
    
    print(f"\n{name} Strategy:")
    print(f"Accuracy: {accuracy:.4f}")
    print("Classification Report:")
    print(classification_report(y_test_m, y_pred_m))

# 4. VISUALIZATION
print("\n" + "="*60)
print("VISUALIZATION")
print("="*60)

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# 1. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
axes[0, 0].set_title('Confusion Matrix')
axes[0, 0].set_xlabel('Predicted')
axes[0, 0].set_ylabel('Actual')

# 2. ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
axes[0, 1].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {roc_auc_score(y_test, y_pred_proba):.3f})')
axes[0, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5)
axes[0, 1].set_xlabel('False Positive Rate')
axes[0, 1].set_ylabel('True Positive Rate')
axes[0, 1].set_title('ROC Curve')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# 3. Precision-Recall vs Threshold
precision_scores = []
recall_scores = []
threshold_values = np.linspace(0, 1, 100)

for threshold in threshold_values:
    y_pred_threshold = (y_pred_proba >= threshold).astype(int)
    if len(np.unique(y_pred_threshold)) > 1:  # Avoid single class predictions
        precision_scores.append(precision_score(y_test, y_pred_threshold, zero_division=0))
        recall_scores.append(recall_score(y_test, y_pred_threshold))
    else:
        precision_scores.append(0)
        recall_scores.append(0)

axes[0, 2].plot(threshold_values, precision_scores, label='Precision', linewidth=2)
axes[0, 2].plot(threshold_values, recall_scores, label='Recall', linewidth=2)
axes[0, 2].axvline(x=0.5, color='r', linestyle='--', alpha=0.5, label='Default Threshold')
axes[0, 2].set_xlabel('Threshold')
axes[0, 2].set_ylabel('Score')
axes[0, 2].set_title('Precision-Recall vs Threshold')
axes[0, 2].legend()
axes[0, 2].grid(True, alpha=0.3)

# 4. Feature Coefficients
axes[1, 0].barh(feature_importance['feature'], feature_importance['coefficient'])
axes[1, 0].set_xlabel('Coefficient Value')
axes[1, 0].set_title('Feature Coefficients')
axes[1, 0].axvline(x=0, color='r', linestyle='--', alpha=0.5)

# 5. Probability Distribution
axes[1, 1].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.5, label='No Churn', color='blue')
axes[1, 1].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.5, label='Churn', color='red')
axes[1, 1].axvline(x=0.5, color='black', linestyle='--', label='Decision Boundary')
axes[1, 1].set_xlabel('Predicted Probability')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].set_title('Probability Distribution by Class')
axes[1, 1].legend()

# 6. Learning Curve
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    log_reg, X_train_scaled, y_train, cv=5,
    train_sizes=np.linspace(0.1, 1.0, 10),
    scoring='roc_auc'
)

axes[1, 2].plot(train_sizes, train_scores.mean(axis=1), label='Training Score', linewidth=2)
axes[1, 2].plot(train_sizes, val_scores.mean(axis=1), label='Validation Score', linewidth=2)
axes[1, 2].fill_between(train_sizes, 
                        train_scores.mean(axis=1) - train_scores.std(axis=1),
                        train_scores.mean(axis=1) + train_scores.std(axis=1),
                        alpha=0.2)
axes[1, 2].fill_between(train_sizes, 
                        val_scores.mean(axis=1) - val_scores.std(axis=1),
                        val_scores.mean(axis=1) + val_scores.std(axis=1),
                        alpha=0.2)
axes[1, 2].set_xlabel('Training Set Size')
axes[1, 2].set_ylabel('ROC-AUC Score')
axes[1, 2].set_title('Learning Curve')
axes[1, 2].legend()
axes[1, 2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nVisualization complete!")
                </div>
            </div>

        </div>

        <!-- Evaluation Section -->
        <div id="evaluation" class="content-section">
            
            <div class="card">
                <h2 class="section-title">
                    <span class="icon-box">📊</span>
                    Model Evaluation Metrics
                </h2>

                <div class="concept-grid">
                    <div class="concept-card">
                        <h4>🎯 Accuracy</h4>
                        <p><strong>Formula:</strong> (TP + TN) / Total</p>
                        <p><strong>Use:</strong> Balanced datasets</p>
                        <p><strong>Limitation:</strong> Misleading for imbalanced data</p>
                    </div>
                    <div class="concept-card">
                        <h4>🔍 Precision</h4>
                        <p><strong>Formula:</strong> TP / (TP + FP)</p>
                        <p><strong>Meaning:</strong> Of predicted positives, how many correct?</p>
                        <p><strong>Focus:</strong> Minimize false positives</p>
                    </div>
                    <div class="concept-card">
                        <h4>📢 Recall (Sensitivity)</h4>
                        <p><strong>Formula:</strong> TP / (TP + FN)</p>
                        <p><strong>Meaning:</strong> Of actual positives, how many found?</p>
                        <p><strong>Focus:</strong> Minimize false negatives</p>
                    </div>
                    <div class="concept-card">
                        <h4>⚖️ F1-Score</h4>
                        <p><strong>Formula:</strong> 2 × (Precision × Recall) / (Precision + Recall)</p>
                        <p><strong>Use:</strong> Balance precision and recall</p>
                        <p><strong>Best for:</strong> Imbalanced datasets</p>
                    </div>
                    <div class="concept-card">
                        <h4>📈 ROC-AUC</h4>
                        <p><strong>ROC:</strong> TPR vs FPR curve</p>
                        <p><strong>AUC:</strong> Area under ROC curve</p>
                        <p><strong>Range:</strong> 0.5 (random) to 1.0 (perfect)</p>
                    </div>
                    <div class="concept-card">
                        <h4>📉 Log Loss</h4>
                        <p><strong>Formula:</strong> -Σ[y·log(ŷ) + (1-y)·log(1-ŷ)]</p>
                        <p><strong>Use:</strong> Probabilistic evaluation</p>
                        <p><strong>Lower is better:</strong> 0 is perfect</p>
                    </div>
                </div>

                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Best When</th>
                            <th>Example Use Case</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Accuracy</td>
                            <td>Classes balanced</td>
                            <td>General classification</td>
                        </tr>
                        <tr>
                            <td>Precision</td>
                            <td>False positives costly</td>
                            <td>Spam detection, fraud detection</td>
                        </tr>
                        <tr>
                            <td>Recall</td>
                            <td>False negatives costly</td>
                            <td>Disease diagnosis, security threats</td>
                        </tr>
                        <tr>
                            <td>F1-Score</td>
                            <td>Need balance</td>
                            <td>Information retrieval</td>
                        </tr>
                        <tr>
                            <td>ROC-AUC</td>
                            <td>Ranking important</td>
                            <td>Credit scoring, risk assessment</td>
                        </tr>
                    </tbody>
                </table>
            </div>

        </div>

        <!-- Real World Section -->
        <div id="realworld" class="content-section">
            
            <div class="card">
                <h2 class="section-title">
                    <span class="icon-box">🌍</span>
                    Real-World Applications
                </h2>

                <div class="real-example">
                    <h4>💳 Credit Card Fraud Detection (Banking)</h4>
                    <p><strong>Problem:</strong> Detect fraudulent transactions in real-time</p>
                    <p><strong>Features:</strong> Transaction amount, location, time, merchant type, user history</p>
                    <p><strong>Challenge:</strong> Extreme class imbalance (0.1% fraud), need high precision</p>
                    <p><strong>Solution:</strong> Weighted logistic regression with cost-sensitive learning</p>
                </div>

                <div class="real-example">
                    <h4>📧 Email Spam Classification (Gmail)</h4>
                    <p><strong>Problem:</strong> Filter spam emails automatically</p>
                    <p><strong>Features:</strong> Word frequencies, sender reputation, links, attachments</p>
                    <p><strong>Challenge:</strong> Evolving spam patterns, multilingual content</p>
                    <p><strong>Solution:</strong> Online learning with continuous model updates</p>
                </div>

                <div class="real-example">
                    <h4>🏥 Disease Diagnosis (Healthcare)</h4>
                    <p><strong>Problem:</strong> Predict diabetes from patient records</p>
                    <p><strong>Features:</strong> BMI, glucose level, blood pressure, age, family history</p>
                    <p><strong>Challenge:</strong> Missing data, need high recall (can't miss cases)</p>
                    <p><strong>Solution:</strong> Ensemble of logistic regression models with imputation</p>
                </div>

                <div class="real-example">
                    <h4>🛍️ Customer Churn Prediction (Netflix/Spotify)</h4>
                    <p><strong>Problem:</strong> Identify users likely to cancel subscription</p>
                    <p><strong>Features:</strong> Usage patterns, content preferences, payment history, engagement</p>
                    <p><strong>Challenge:</strong> Time-dependent features, seasonal patterns</p>
                    <p><strong>Solution:</strong> Time-aware logistic regression with feature engineering</p>
                </div>

                <div class="real-example">
                    <h4>🎯 Ad Click Prediction (Google/Facebook)</h4>
                    <p><strong>Problem:</strong> Predict probability of user clicking an ad</p>
                    <p><strong>Features:</strong> User demographics, browsing history, ad content, context</p>
                    <p><strong>Challenge:</strong> Billions of predictions per day, real-time requirements</p>
                    <p><strong>Solution:</strong> Distributed logistic regression with feature hashing</p>
                </div>

                <div class="code-block">
# Real-World Example: Credit Card Fraud Detection
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

# Simulate credit card transaction data
np.random.seed(42)
n_transactions = 10000
fraud_rate = 0.002  # 0.2% fraud

# Generate features
data = {
    'amount': np.random.exponential(50, n_transactions),
    'days_since_last': np.random.exponential(1, n_transactions),
    'num_transactions_today': np.random.poisson(3, n_transactions),
    'merchant_risk_score': np.random.uniform(0, 1, n_transactions),
    'unusual_time': np.random.binomial(1, 0.1, n_transactions),
    'foreign_transaction': np.random.binomial(1, 0.05, n_transactions),
}

df = pd.DataFrame(data)

# Create fraud labels (with patterns)
fraud_probability = (
    0.001 +  # Base rate
    0.01 * (df['amount'] > 500) +  # Large amounts
    0.02 * df['unusual_time'] +  # Unusual times
    0.03 * df['foreign_transaction'] +  # Foreign transactions
    0.05 * (df['merchant_risk_score'] > 0.8)  # Risky merchants
)
df['is_fraud'] = np.random.binomial(1, fraud_probability.clip(0, 1))

# Ensure we have at least some fraud cases
df.loc[np.random.choice(df.index, 20), 'is_fraud'] = 1

print("Credit Card Fraud Detection Dataset")
print("="*40)
print(f"Total transactions: {len(df)}")
print(f"Fraud transactions: {df['is_fraud'].sum()}")
print(f"Fraud rate: {df['is_fraud'].mean():.2%}")

# Prepare features and target
X = df.drop('is_fraud', axis=1)
y = df['is_fraud']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Handle class imbalance with SMOTE
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

print(f"\nAfter SMOTE balancing:")
print(f"Training samples: {len(y_train_balanced)}")
print(f"Fraud rate: {y_train_balanced.mean():.2%}")

# Train models with different strategies
models = {
    'Standard': LogisticRegression(random_state=42),
    'Balanced': LogisticRegression(class_weight='balanced', random_state=42),
    'SMOTE': LogisticRegression(random_state=42)
}

# Train standard and balanced on original data
models['Standard'].fit(X_train_scaled, y_train)
models['Balanced'].fit(X_train_scaled, y_train)

# Train SMOTE model on balanced data
models['SMOTE'].fit(X_train_balanced, y_train_balanced)

# Evaluate models
from sklearn.metrics import classification_report, precision_score, recall_score

print("\n" + "="*60)
print("MODEL COMPARISON")
print("="*60)

for name, model in models.items():
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    
    print(f"\n{name} Model:")
    print(f"Precision: {precision_score(y_test, y_pred):.3f}")
    print(f"Recall: {recall_score(y_test, y_pred):.3f}")
    
    # Find optimal threshold for high precision
    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)
    
    # Find threshold for 95% precision
    high_precision_idx = np.where(precision >= 0.95)[0]
    if len(high_precision_idx) > 0:
        optimal_threshold = thresholds[high_precision_idx[0]]
        y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)
        
        print(f"With 95% precision threshold ({optimal_threshold:.3f}):")
        print(f"  Precision: {precision_score(y_test, y_pred_optimal):.3f}")
        print(f"  Recall: {recall_score(y_test, y_pred_optimal):.3f}")

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Missing Value Imputation - Complete Guide with Real Examples</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
        }

        .header {
            text-align: center;
            padding: 60px 30px;
            background: rgba(255, 255, 255, 0.98);
            border-radius: 30px;
            margin-bottom: 40px;
            box-shadow: 0 30px 60px rgba(0, 0, 0, 0.15);
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 5px;
            background: linear-gradient(90deg, #3b82f6, #22c55e, #f59e0b, #ef4444);
            animation: shimmer 3s infinite;
        }

        @keyframes shimmer {
            0% { transform: translateX(-100%); }
            100% { transform: translateX(100%); }
        }

        .header h1 {
            font-size: 3em;
            background: linear-gradient(135deg, #3b82f6 0%, #22c55e 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 15px;
            animation: fadeInUp 0.8s ease;
        }

        .nav-tabs {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin-bottom: 40px;
            justify-content: center;
        }

        .nav-tab {
            padding: 15px 25px;
            background: white;
            border: none;
            border-radius: 50px;
            cursor: pointer;
            font-size: 1.05em;
            font-weight: 600;
            transition: all 0.3s ease;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        .nav-tab:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 15px rgba(0, 0, 0, 0.2);
        }

        .nav-tab.active {
            background: linear-gradient(135deg, #3b82f6 0%, #22c55e 100%);
            color: white;
        }

        .content-section {
            display: none;
        }

        .content-section.active {
            display: block;
            animation: fadeInUp 0.6s ease;
        }

        .card {
            background: white;
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 15px 35px rgba(0, 0, 0, 0.1);
            transition: all 0.3s ease;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
        }

        .section-title {
            font-size: 2em;
            margin-bottom: 25px;
            color: #1a1a1a;
            display: flex;
            align-items: center;
            gap: 15px;
        }

        .icon-box {
            width: 50px;
            height: 50px;
            background: linear-gradient(135deg, #3b82f6, #22c55e);
            border-radius: 15px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 24px;
        }

        .visual-demo {
            background: linear-gradient(135deg, #f5f7fa 0%, #e9ecef 100%);
            border-radius: 15px;
            padding: 30px;
            margin: 25px 0;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        .technique-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }

        .technique-card {
            background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%);
            border-left: 4px solid #3b82f6;
            border-radius: 10px;
            padding: 20px;
            transition: transform 0.3s ease;
        }

        .technique-card:hover {
            transform: translateX(5px);
        }

        .technique-card h4 {
            color: #1e40af;
            margin-bottom: 10px;
            font-size: 1.2em;
        }

        .code-block {
            background: #1e293b;
            color: #e2e8f0;
            border-radius: 15px;
            padding: 25px;
            margin: 25px 0;
            overflow-x: auto;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.95em;
            line-height: 1.8;
            position: relative;
        }

        .code-block::before {
            content: 'Python';
            position: absolute;
            top: 10px;
            right: 10px;
            background: rgba(99, 102, 241, 0.2);
            color: #818cf8;
            padding: 4px 12px;
            border-radius: 6px;
            font-size: 0.8em;
        }

        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 25px 0;
        }

        .pros, .cons {
            padding: 20px;
            border-radius: 15px;
        }

        .pros {
            background: linear-gradient(135deg, #d4f4dd 0%, #bbf7d0 100%);
            border: 2px solid #22c55e;
        }

        .cons {
            background: linear-gradient(135deg, #fee2e2 0%, #fecaca 100%);
            border: 2px solid #ef4444;
        }

        .warning-box {
            background: linear-gradient(135deg, #fef3c7, #fde68a);
            border: 2px solid #f59e0b;
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
            display: flex;
            align-items: start;
            gap: 15px;
        }

        .example-box {
            background: linear-gradient(135deg, #e0e7ff, #c7d2fe);
            border-radius: 15px;
            padding: 25px;
            margin: 25px 0;
            border: 2px solid #6366f1;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }

        .comparison-table th {
            background: linear-gradient(135deg, #3b82f6, #22c55e);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }

        .comparison-table td {
            padding: 15px;
            border-bottom: 1px solid #e5e7eb;
        }

        .comparison-table tr:hover {
            background: #f8fafc;
        }

        .missing-pattern {
            display: grid;
            grid-template-columns: repeat(10, 1fr);
            gap: 3px;
            margin: 20px 0;
            padding: 15px;
            background: #f8f9fa;
            border-radius: 10px;
        }

        .data-cell {
            width: 100%;
            aspect-ratio: 1;
            border-radius: 3px;
            transition: transform 0.3s ease;
        }

        .data-cell:hover {
            transform: scale(1.2);
        }

        .data-present {
            background: #3b82f6;
        }

        .data-missing {
            background: #ef4444;
        }

        .data-imputed {
            background: #22c55e;
        }

        .legend {
            display: flex;
            gap: 20px;
            margin: 15px 0;
            justify-content: center;
        }

        .legend-item {
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .legend-color {
            width: 20px;
            height: 20px;
            border-radius: 3px;
        }

        .metric-display {
            display: flex;
            justify-content: space-around;
            margin: 20px 0;
            flex-wrap: wrap;
            gap: 20px;
        }

        .metric-card {
            text-align: center;
            padding: 20px;
            background: white;
            border-radius: 15px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            min-width: 150px;
        }

        .metric-value {
            font-size: 2em;
            font-weight: bold;
            margin: 10px 0;
        }

        .good { color: #22c55e; }
        .warning { color: #f59e0b; }
        .bad { color: #ef4444; }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        @media (max-width: 768px) {
            .header h1 { font-size: 2em; }
            .technique-grid { grid-template-columns: 1fr; }
            .pros-cons { grid-template-columns: 1fr; }
        }
    </style>
    <script>
        function showSection(event, sectionId) {
            // Hide all sections
            const sections = document.querySelectorAll('.content-section');
            sections.forEach(section => {
                section.classList.remove('active');
            });
            
            // Show selected section
            const selectedSection = document.getElementById(sectionId);
            if (selectedSection) {
                selectedSection.classList.add('active');
            }
            
            // Update tab styling
            const tabs = document.querySelectorAll('.nav-tab');
            tabs.forEach(tab => {
                tab.classList.remove('active');
            });
            
            // Add active class to clicked tab
            if (event && event.target) {
                event.target.classList.add('active');
            }
        }
    </script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>🔧 Missing Value Imputation</h1>
            <p>Complete Guide with Real-World Examples & Best Practices</p>
        </div>

        <div class="nav-tabs">
            <button class="nav-tab active" onclick="showSection(event, 'overview')">📊 Overview</button>
            <button class="nav-tab" onclick="showSection(event, 'patterns')">🔍 Missing Patterns</button>
            <button class="nav-tab" onclick="showSection(event, 'simple')">🎯 Simple Methods</button>
            <button class="nav-tab" onclick="showSection(event, 'advanced')">🚀 Advanced Methods</button>
            <button class="nav-tab" onclick="showSection(event, 'implementation')">💻 Implementation</button>
            <button class="nav-tab" onclick="showSection(event, 'realworld')">🌍 Real Examples</button>
        </div>

        <!-- Overview Section -->
        <div id="overview" class="content-section active">
            
            <div class="card">
                <h2 class="section-title">
                    <span class="icon-box">📊</span>
                    What is Missing Value Imputation?
                </h2>
                
                <p style="font-size: 1.2em; line-height: 1.8; color: #475569; margin-bottom: 20px;">
                    Missing value imputation is the process of replacing missing data with substituted values. 
                    In real-world datasets, missing data is common due to sensor failures, survey non-responses, 
                    data entry errors, or privacy concerns.
                </p>

                <div class="visual-demo">
                    <svg width="600" height="300" viewBox="0 0 600 300">
                        <!-- Original Data with Missing Values -->
                        <g transform="translate(50, 50)">
                            <text x="100" y="-10" text-anchor="middle" font-weight="bold" fill="#333">Original Data</text>
                            <rect x="0" y="0" width="200" height="40" fill="#3b82f6"/>
                            <rect x="0" y="45" width="200" height="40" fill="#ef4444" stroke-dasharray="5,5" fill-opacity="0.3"/>
                            <rect x="0" y="90" width="200" height="40" fill="#3b82f6"/>
                            <rect x="0" y="135" width="200" height="40" fill="#ef4444" stroke-dasharray="5,5" fill-opacity="0.3"/>
                            <rect x="0" y="180" width="200" height="40" fill="#3b82f6"/>
                            
                            <text x="100" y="25" text-anchor="middle" fill="white">Data: 25.3</text>
                            <text x="100" y="70" text-anchor="middle" fill="#ef4444">Missing: NaN</text>
                            <text x="100" y="115" text-anchor="middle" fill="white">Data: 28.7</text>
                            <text x="100" y="160" text-anchor="middle" fill="#ef4444">Missing: NaN</text>
                            <text x="100" y="205" text-anchor="middle" fill="white">Data: 24.1</text>
                        </g>

                        <!-- Arrow -->
                        <path d="M 280 150 L 320 150" stroke="#666" stroke-width="3" marker-end="url(#arrowhead)"/>
                        <defs>
                            <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                <polygon points="0 0, 10 3, 0 6" fill="#666"/>
                            </marker>
                        </defs>
                        
                        <!-- Imputed Data -->
                        <g transform="translate(350, 50)">
                            <text x="100" y="-10" text-anchor="middle" font-weight="bold" fill="#333">After Imputation</text>
                            <rect x="0" y="0" width="200" height="40" fill="#3b82f6"/>
                            <rect x="0" y="45" width="200" height="40" fill="#22c55e"/>
                            <rect x="0" y="90" width="200" height="40" fill="#3b82f6"/>
                            <rect x="0" y="135" width="200" height="40" fill="#22c55e"/>
                            <rect x="0" y="180" width="200" height="40" fill="#3b82f6"/>
                            
                            <text x="100" y="25" text-anchor="middle" fill="white">Data: 25.3</text>
                            <text x="100" y="70" text-anchor="middle" fill="white">Imputed: 26.0</text>
                            <text x="100" y="115" text-anchor="middle" fill="white">Data: 28.7</text>
                            <text x="100" y="160" text-anchor="middle" fill="white">Imputed: 26.4</text>
                            <text x="100" y="205" text-anchor="middle" fill="white">Data: 24.1</text>
                        </g>
                    </svg>
                </div>

                <div class="legend">
                    <div class="legend-item">
                        <div class="legend-color" style="background: #3b82f6;"></div>
                        <span>Original Data</span>
                    </div>
                    <div class="legend-item">
                        <div class="legend-color" style="background: #ef4444; opacity: 0.3;"></div>
                        <span>Missing Values</span>
                    </div>
                    <div class="legend-item">
                        <div class="legend-color" style="background: #22c55e;"></div>
                        <span>Imputed Values</span>
                    </div>
                </div>

                <div class="metric-display">
                    <div class="metric-card">
                        <div class="metric-value bad">30-70%</div>
                        <div>Typical missing rate in medical data</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value warning">10-30%</div>
                        <div>Survey response data</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value good">1-5%</div>
                        <div>Well-maintained databases</div>
                    </div>
                </div>
            </div>

            <div class="card">
                <h2 class="section-title">
                    <span class="icon-box">⚠️</span>
                    Why Missing Values are a Problem
                </h2>

                <div class="technique-grid">
                    <div class="technique-card">
                        <h4>🚫 Most ML Algorithms Can't Handle NaN</h4>
                        <p>Algorithms like Linear Regression, SVM, Neural Networks will throw errors with missing values.</p>
                    </div>
                    <div class="technique-card">
                        <h4>📉 Reduced Sample Size</h4>
                        <p>Dropping rows with missing values can eliminate significant portions of your dataset.</p>
                    </div>
                    <div class="technique-card">
                        <h4>⚖️ Bias Introduction</h4>
                        <p>If missingness is not random, dropping data can introduce systematic bias.</p>
                    </div>
                    <div class="technique-card">
                        <h4>📊 Loss of Statistical Power</h4>
                        <p>Fewer observations mean wider confidence intervals and less reliable predictions.</p>
                    </div>
                    <div class="technique-card">
                        <h4>🔄 Distorted Relationships</h4>
                        <p>Missing values can mask or exaggerate relationships between variables.</p>
                    </div>
                    <div class="technique-card">
                        <h4>💰 Wasted Resources</h4>
                        <p>Collected data that can't be used represents wasted time and money.</p>
                    </div>
                </div>
            </div>

        </div>

        <!-- Missing Patterns Section -->
        <div id="patterns" class="content-section">
            
            <div class="card">
                <h2 class="section-title">
                    <span class="icon-box">🔍</span>
                    Types of Missing Data Patterns
                </h2>

                <div class="technique-grid">
                    <div class="technique-card">
                        <h4>1️⃣ MCAR (Missing Completely At Random)</h4>
                        <p><strong>Definition:</strong> Missingness is unrelated to any values</p>
                        <p><strong>Example:</strong> Survey pages randomly lost in mail</p>
                        <p><strong>Test:</strong> Little's MCAR test</p>
                        <p><strong>Impact:</strong> Least problematic, any method works</p>
                    </div>
                    
                    <div class="technique-card">
                        <h4>2️⃣ MAR (Missing At Random)</h4>
                        <p><strong>Definition:</strong> Missingness depends on observed data</p>
                        <p><strong>Example:</strong> Men less likely to report weight</p>
                        <p><strong>Test:</strong> Analyze patterns by groups</p>
                        <p><strong>Impact:</strong> Need sophisticated imputation</p>
                    </div>
                    
                    <div class="technique-card">
                        <h4>3️⃣ MNAR (Missing Not At Random)</h4>
                        <p><strong>Definition:</strong> Missingness depends on missing value itself</p>
                        <p><strong>Example:</strong> High earners skip income question</p>
                        <p><strong>Test:</strong> Domain knowledge required</p>
                        <p><strong>Impact:</strong> Most problematic, needs special handling</p>
                    </div>
                </div>

                <div class="visual-demo">
                    <div style="display: flex; justify-content: space-around; align-items: center;">
                        <div style="text-align: center;">
                            <h4>MCAR Pattern</h4>
                            <div class="missing-pattern">
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                            </div>
                            <p style="margin-top: 10px; color: #666;">Random distribution</p>
                        </div>
                        
                        <div style="text-align: center;">
                            <h4>MAR Pattern</h4>
                            <div class="missing-pattern">
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-missing"></div>
                            </div>
                            <p style="margin-top: 10px; color: #666;">Pattern by groups</p>
                        </div>
                        
                        <div style="text-align: center;">
                            <h4>MNAR Pattern</h4>
                            <div class="missing-pattern">
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-missing"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                                <div class="data-cell data-present"></div>
                            </div>
                            <p style="margin-top: 10px; color: #666;">Systematic pattern</p>
                        </div>
                    </div>
                </div>

                <div class="code-block">
# Detecting Missing Value Patterns
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

# Create sample data with different missing patterns
np.random.seed(42)
n = 1000

# MCAR: Completely random
data_mcar = pd.DataFrame({
    'age': np.random.normal(40, 15, n),
    'income': np.random.normal(50000, 20000, n),
    'health_score': np.random.normal(75, 10, n)
})
# Randomly remove 20% of income values
mcar_mask = np.random.random(n) < 0.2
data_mcar.loc[mcar_mask, 'income'] = np.nan

# MAR: Missing depends on observed variable
data_mar = data_mcar.copy()
# Younger people less likely to report health score
mar_mask = (data_mar['age'] < 30) & (np.random.random(n) < 0.5)
data_mar.loc[mar_mask, 'health_score'] = np.nan

# MNAR: Missing depends on the value itself
data_mnar = data_mcar.copy()
# High earners don't report income
mnar_mask = data_mnar['income'] > 70000
data_mnar.loc[mnar_mask & (np.random.random(n) < 0.7), 'income'] = np.nan

# Visualize missing patterns
def plot_missing_pattern(df, title):
    # Create missing value matrix
    missing_matrix = df.isnull().astype(int)
    
    plt.figure(figsize=(10, 6))
    sns.heatmap(missing_matrix.head(50), cbar=True, 
                yticklabels=False, cmap='RdYlBu', 
                vmin=0, vmax=1)
    plt.title(f'{title} - Missing Value Pattern (1=Missing, 0=Present)')
    plt.tight_layout()
    plt.show()
    
    # Summary statistics
    print(f"\n{title} Missing Value Summary:")
    print(df.isnull().sum())
    print(f"Total missing: {df.isnull().sum().sum()}/{df.size} ({df.isnull().sum().sum()/df.size*100:.1f}%)")

# Test for MCAR using Little's test (simplified version)
def simple_mcar_test(df):
    """Simplified MCAR test based on correlation patterns"""
    # Check if missingness in one variable correlates with values in others
    correlations = []
    for col in df.columns:
        if df[col].isnull().any():
            for other_col in df.columns:
                if col != other_col:
                    # Correlate missingness with other variables
                    mask = ~df[other_col].isnull()
                    if mask.sum() > 0:
                        corr = stats.pointbiserialr(
                            df[col].isnull()[mask], 
                            df[other_col][mask].fillna(df[other_col].mean())
                        )[0]
                        correlations.append(abs(corr))
    
    max_corr = max(correlations) if correlations else 0
    if max_corr < 0.1:
        return "Likely MCAR (random pattern)"
    elif max_corr < 0.3:
        return "Likely MAR (some pattern)"
    else:
        return "Likely MNAR (strong pattern)"

# Analyze patterns
print("Pattern Analysis:")
print(f"MCAR Data: {simple_mcar_test(data_mcar)}")
print(f"MAR Data: {simple_mcar_test(data_mar)}")
print(f"MNAR Data: {simple_mcar_test(data_mnar)}")
                </div>
            </div>

        </div>

        <!-- Simple Methods Section -->
        <div id="simple" class="content-section">
            
            <div class="card">
                <h2 class="section-title">
                    <span class="icon-box">🎯</span>
                    Simple Imputation Methods
                </h2>

                <div class="technique-grid">
                    <div class="technique-card">
                        <h4>1. Deletion Methods</h4>
                        <p><strong>Listwise Deletion:</strong> Remove entire rows with any missing values</p>
                        <p><strong>Pairwise Deletion:</strong> Use available data for each analysis</p>
                        <div class="pros-cons" style="margin-top: 15px;">
                            <div class="pros">
                                <strong>Pros:</strong> Simple, no assumptions
                            </div>
                            <div class="cons">
                                <strong>Cons:</strong> Data loss, bias if not MCAR
                            </div>
                        </div>
                    </div>

                    <div class="technique-card">
                        <h4>2. Mean/Median/Mode Imputation</h4>
                        <p>Replace missing values with the central tendency</p>
                        <p><strong>Mean:</strong> For normal distributions</p>
                        <p><strong>Median:</strong> For skewed distributions</p>
                        <p><strong>Mode:</strong> For categorical variables</p>
                        <div class="pros-cons" style="margin-top: 15px;">
                            <div class="pros">
                                <strong>Pros:</strong> Fast, preserves sample size
                            </div>
                            <div class="cons">
                                <strong>Cons:</strong> Reduces variance, ignores relationships
                            </div>
                        </div>
                    </div>

                    <div class="technique-card">
                        <h4>3. Forward/Backward Fill</h4>
                        <p>Use previous or next value (time series)</p>
                        <p><strong>Forward:</strong> Carry last observation forward</p>
                        <p><strong>Backward:</strong> Use next observation</p>
                        <div class="pros-cons" style="margin-top: 15px;">
                            <div class="pros">
                                <strong>Pros:</strong> Good for time series
                            </div>
                            <div class="cons">
                                <strong>Cons:</strong> Only works for temporal data
                            </div>
                        </div>
                    </div>

                    <div class="technique-card">
                        <h4>4. Constant Value Imputation</h4>
                        <p>Replace with a specific value</p>
                        <p><strong>Zero:</strong> When missing means absence</p>
                        <p><strong>-999:</strong> Flag for "unknown"</p>
                        <p><strong>Domain value:</strong> Business logic</p>
                        <div class="pros-cons" style="margin-top: 15px;">
                            <div class="pros">
                                <strong>Pros:</strong> Clear interpretation
                            </div>
                            <div class="cons">
                                <strong>Cons:</strong> Can distort distribution
                            </div>
                        </div>
                    </div>
                </div>

                <div class="code-block">
# Simple Imputation Methods Implementation
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# Create sample dataset with missing values
np.random.seed(42)
df = pd.DataFrame({
    'age': [25, 30, np.nan, 45, 50, np.nan, 35, 40, 28, np.nan],
    'salary': [50000, 60000, 70000, np.nan, 90000, 85000, np.nan, 75000, 55000, 65000],
    'experience': [2, 5, 7, np.nan, 15, 12, 8, np.nan, 3, 6],
    'department': ['IT', 'HR', np.nan, 'IT', 'Finance', 'HR', np.nan, 'IT', 'Finance', 'HR'],
    'satisfaction': [4, 5, 3, np.nan, 5, 4, 3, np.nan, 4, 5]
})

print("Original Data:")
print(df)
print(f"\nMissing Values:\n{df.isnull().sum()}")

# 1. Deletion Methods
print("\n" + "="*60)
print("1. DELETION METHODS")
print("="*60)

# Listwise deletion (complete case analysis)
df_complete = df.dropna()
print(f"Listwise Deletion: {len(df)} rows → {len(df_complete)} rows")
print(f"Data Loss: {(1 - len(df_complete)/len(df))*100:.1f}%")

# 2. Mean/Median/Mode Imputation
print("\n" + "="*60)
print("2. MEAN/MEDIAN/MODE IMPUTATION")
print("="*60)

# Mean imputation for numerical
df_mean = df.copy()
df_mean['age'].fillna(df_mean['age'].mean(), inplace=True)
df_mean['salary'].fillna(df_mean['salary'].mean(), inplace=True)

# Median imputation for skewed data
df_median = df.copy()
df_median['experience'].fillna(df_median['experience'].median(), inplace=True)

# Mode imputation for categorical
df_mode = df.copy()
df_mode['department'].fillna(df_mode['department'].mode()[0], inplace=True)

print(f"Mean Age: {df_mean['age'].mean():.1f}")
print(f"Median Experience: {df_median['experience'].median():.1f}")
print(f"Mode Department: {df_mode['department'].mode()[0]}")

# Using Scikit-learn's SimpleImputer
print("\n" + "="*60)
print("USING SCIKIT-LEARN")
print("="*60)

# Numerical features
numerical_features = ['age', 'salary', 'experience', 'satisfaction']
categorical_features = ['department']

# Mean imputation
imputer_mean = SimpleImputer(strategy='mean')
df[numerical_features] = imputer_mean.fit_transform(df[numerical_features])

# Most frequent for categorical
imputer_mode = SimpleImputer(strategy='most_frequent')
df[categorical_features] = imputer_mode.fit_transform(df[categorical_features])

print("After Imputation:")
print(df)

# 3. Forward/Backward Fill (Time Series)
print("\n" + "="*60)
print("3. FORWARD/BACKWARD FILL")
print("="*60)

# Create time series data
dates = pd.date_range('2024-01-01', periods=10, freq='D')
ts_data = pd.DataFrame({
    'date': dates,
    'temperature': [20, 22, np.nan, 25, np.nan, np.nan, 28, 30, np.nan, 32],
    'sales': [100, 110, np.nan, 130, np.nan, 150, np.nan, 170, 180, np.nan]
})

print("Original Time Series:")
print(ts_data)

# Forward fill
ts_ffill = ts_data.copy()
ts_ffill['temperature'] = ts_ffill['temperature'].ffill()
print("\nForward Fill:")
print(ts_ffill)

# Backward fill
ts_bfill = ts_data.copy()
ts_bfill['sales'] = ts_bfill['sales'].bfill()
print("\nBackward Fill:")
print(ts_bfill)

# Linear interpolation for time series
ts_interp = ts_data.copy()
ts_interp['temperature'] = ts_interp['temperature'].interpolate(method='linear')
ts_interp['sales'] = ts_interp['sales'].interpolate(method='linear')
print("\nLinear Interpolation:")
print(ts_interp)

# 4. Domain-Specific Imputation
print("\n" + "="*60)
print("4. DOMAIN-SPECIFIC IMPUTATION")
print("="*60)

# Example: Survey data where missing might mean "No" or "0"
survey_data = pd.DataFrame({
    'owns_car': [1, 1, np.nan, 0, 1, np.nan],  # Missing might mean "No"
    'num_children': [2, 0, np.nan, 3, 1, np.nan],  # Missing might mean "0"
    'income_bracket': ['high', 'medium', np.nan, 'low', np.nan, 'high']  # Missing might mean "prefer not to say"
})

print("Original Survey Data:")
print(survey_data)

# Domain-specific imputation
survey_imputed = survey_data.copy()
survey_imputed['owns_car'].fillna(0, inplace=True)  # Assume no car if not mentioned
survey_imputed['num_children'].fillna(0, inplace=True)  # Assume no children if not mentioned
survey_imputed['income_bracket'].fillna('not_disclosed', inplace=True)  # Special category

print("\nDomain-Specific Imputation:")
print(survey_imputed)
                </div>

                <div class="warning-box">
                    <span style="font-size: 24px;">⚠️</span>
                    <div>
                        <strong>Important:</strong> Simple methods can introduce bias and reduce variance. 
                        They don't account for relationships between variables and can lead to underestimated standard errors.
                        Use them only when missing data is minimal (&lt;5%) and MCAR.
                    </div>
                </div>
            </div>

        </div>

        <!-- Advanced Methods Section -->
        <div id="advanced" class="content-section">
            
            <div class="card">
                <h2 class="section-title">
                    <span class="icon-box">🚀</span>
                    Advanced Imputation Methods
                </h2>

                <div class="technique-grid">
                    <div class="technique-card">
                        <h4>🤖 KNN Imputation</h4>
                        <p>Uses K nearest neighbors to impute based on similar records</p>
                        <p><strong>Best for:</strong> When similar records have similar values</p>
                        <p><strong>Pros:</strong> Considers multiple features</p>
                        <p><strong>Cons:</strong> Computationally expensive for large data</p>
                    </div>

                    <div class="technique-card">
                        <h4>🌲 Random Forest Imputation</h4>
                        <p>Uses Random Forest to predict missing values</p>
                        <p><strong>Best for:</strong> Complex non-linear relationships</p>
                        <p><strong>Pros:</strong> Handles mixed types, robust</p>
                        <p><strong>Cons:</strong> Can overfit, slow</p>
                    </div>

                    <div class="technique-card">
                        <h4>🔄 MICE (Multiple Imputation)</h4>
                        <p>Creates multiple imputed datasets and pools results</p>
                        <p><strong>Best for:</strong> Statistical inference</p>
                        <p><strong>Pros:</strong> Accounts for uncertainty</p>
                        <p><strong>Cons:</strong> Complex, computationally intensive</p>
                    </div>

                    <div class="technique-card">
                        <h4>🧠 Deep Learning (Autoencoders)</h4>
                        <p>Neural networks learn data representation</p>
                        <p><strong>Best for:</strong> Large complex datasets</p>
                        <p><strong>Pros:</strong> Captures complex patterns</p>
                        <p><strong>Cons:</strong> Requires lots of data, black box</p>
                    </div>

                    <div class="technique-card">
                        <h4>🎯 Matrix Factorization</h4>
                        <p>Decomposes data matrix into latent factors</p>
                        <p><strong>Best for:</strong> Recommendation systems</p>
                        <p><strong>Pros:</strong> Good for sparse data</p>
                        <p><strong>Cons:</strong> Assumes low rank structure</p>
                    </div>

                    <div class="technique-card">
                        <h4>📊 EM Algorithm</h4>
                        <p>Expectation-Maximization for missing data</p>
                        <p><strong>Best for:</strong> Gaussian mixture models</p>
                        <p><strong>Pros:</strong> Theoretically sound</p>
                        <p><strong>Cons:</strong> Assumes specific distributions</p>
                    </div>
                </div>

                <div class="code-block">
# Advanced Imputation Methods
import pandas as pd
import numpy as np
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import KNNImputer, IterativeImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
import warnings
warnings.filterwarnings('ignore')

# Create a more complex dataset
np.random.seed(42)
n = 1000

# Generate correlated features
mean = [50, 60000, 10, 75]
cov = [[100, 5000, 30, 20],
       [5000, 1e8, 1000, 500],
       [30, 1000, 25, 10],
       [20, 500, 10, 100]]

data = np.random.multivariate_normal(mean, cov, n)
df = pd.DataFrame(data, columns=['age', 'income', 'experience', 'satisfaction'])

# Introduce missing values with pattern
# Younger people less likely to report income (MAR)
missing_mask = np.random.random(n) < 0.3
young_mask = df['age'] < 40
df.loc[missing_mask & young_mask, 'income'] = np.nan

# Random missing for other variables
for col in ['age', 'experience', 'satisfaction']:
    mask = np.random.random(n) < 0.2
    df.loc[mask, col] = np.nan

print("Dataset with Missing Values:")
print(df.info())
print(f"\nMissing Values:\n{df.isnull().sum()}")

# Store original complete data for comparison
df_complete = df.dropna()
X_true = df_complete.values

# 1. KNN IMPUTATION
print("\n" + "="*60)
print("1. KNN IMPUTATION")
print("="*60)

# Try different K values
k_values = [3, 5, 7, 10]
knn_results = {}

for k in k_values:
    imputer = KNNImputer(n_neighbors=k)
    df_knn = pd.DataFrame(
        imputer.fit_transform(df),
        columns=df.columns
    )
    
    # Calculate imputation error (for rows that weren't originally missing)
    mask = ~df.isnull().any(axis=1)
    if mask.sum() > 0:
        mse = np.mean((df_knn.loc[mask] - df.loc[mask])**2)
        knn_results[k] = mse.mean()
        print(f"K={k}: MSE = {mse.mean():.2f}")

# Best K
best_k = min(knn_results, key=knn_results.get)
print(f"\nBest K: {best_k}")

# 2. ITERATIVE IMPUTATION (MICE)
print("\n" + "="*60)
print("2. ITERATIVE IMPUTATION (MICE)")
print("="*60)

# Using different estimators
estimators = {
    'Linear': LinearRegression(),
    'RandomForest': RandomForestRegressor(n_estimators=10, random_state=42)
}

for name, estimator in estimators.items():
    imputer = IterativeImputer(
        estimator=estimator,
        max_iter=10,
        random_state=42
    )
    
    df_mice = pd.DataFrame(
        imputer.fit_transform(df),
        columns=df.columns
    )
    
    print(f"\n{name} Estimator:")
    print(f"Mean values after imputation:")
    print(df_mice.mean())

# 3. MULTIPLE IMPUTATION
print("\n" + "="*60)
print("3. MULTIPLE IMPUTATION")
print("="*60)

# Create multiple imputed datasets
n_imputations = 5
imputed_datasets = []

for i in range(n_imputations):
    imputer = IterativeImputer(
        random_state=i,
        max_iter=10
    )
    df_imputed = pd.DataFrame(
        imputer.fit_transform(df),
        columns=df.columns
    )
    imputed_datasets.append(df_imputed)

# Pool results (Rubin's rules)
# Calculate mean and variance across imputations
pooled_mean = pd.concat(imputed_datasets).groupby(level=0).mean()
pooled_std = pd.concat(imputed_datasets).groupby(level=0).std()

print(f"Created {n_imputations} imputed datasets")
print(f"Pooled means:\n{pooled_mean.mean()}")
print(f"Pooled std (uncertainty):\n{pooled_std.mean()}")

# 4. CUSTOM DEEP LEARNING IMPUTATION (Autoencoder)
print("\n" + "="*60)
print("4. DEEP LEARNING IMPUTATION (AUTOENCODER)")
print("="*60)

try:
    import tensorflow as tf
    from tensorflow import keras
    
    # Prepare data
    df_normalized = (df - df.mean()) / df.std()
    df_normalized = df_normalized.fillna(0)  # Initial fill for training
    
    # Build autoencoder
    input_dim = df.shape[1]
    encoding_dim = 2
    
    encoder = keras.Sequential([
        keras.layers.Dense(8, activation='relu', input_shape=(input_dim,)),
        keras.layers.Dense(encoding_dim, activation='relu')
    ])
    
    decoder = keras.Sequential([
        keras.layers.Dense(8, activation='relu', input_shape=(encoding_dim,)),
        keras.layers.Dense(input_dim, activation='linear')
    ])
    
    autoencoder = keras.Sequential([encoder, decoder])
    autoencoder.compile(optimizer='adam', loss='mse')
    
    # Train
    autoencoder.fit(
        df_normalized, df_normalized,
        epochs=50,
        batch_size=32,
        verbose=0
    )
    
    # Impute
    df_autoencoder = pd.DataFrame(
        autoencoder.predict(df_normalized),
        columns=df.columns
    )
    
    # Denormalize
    df_autoencoder = df_autoencoder * df.std() + df.mean()
    
    print("Autoencoder Imputation Complete")
    print(f"Mean values: {df_autoencoder.mean()}")
    
except ImportError:
    print("TensorFlow not installed. Skipping autoencoder example.")

# 5. COMPARISON OF METHODS
print("\n" + "="*60)
print("5. COMPARISON OF METHODS")
print("="*60)

from sklearn.metrics import mean_squared_error, r2_score

# Create a test case with known values
df_test = df.copy()
# Randomly remove some complete rows' values
complete_indices = df_test.dropna().index
test_indices = np.random.choice(complete_indices, size=min(50, len(complete_indices)), replace=False)

# Store true values
true_values = df_test.loc[test_indices].copy()

# Create missing values
for idx in test_indices:
    col = np.random.choice(df_test.columns)
    df_test.loc[idx, col] = np.nan

# Apply different imputation methods
methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'KNN-5': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(random_state=42)
}

results = {}
for name, imputer in methods.items():
    df_imputed = pd.DataFrame(
        imputer.fit_transform(df_test),
        columns=df_test.columns
    )
    
    # Calculate error only for imputed values
    mse = mean_squared_error(
        true_values.values.flatten(),
        df_imputed.loc[test_indices].values.flatten()
    )
    results[name] = mse
    print(f"{name}: MSE = {mse:.2f}")

# Best method
best_method = min(results, key=results.get)
print(f"\nBest Method: {best_method}")
                </div>

                <div class="example-box">
                    <h4>📊 Choosing the Right Method</h4>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Method</th>
                                <th>Best When</th>
                                <th>Avoid When</th>
                                <th>Complexity</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Mean/Median</td>
                                <td>MCAR, &lt;5% missing</td>
                                <td>Relationships exist</td>
                                <td>Low</td>
                            </tr>
                            <tr>
                                <td>KNN</td>
                                <td>Similar records cluster</td>
                                <td>High dimensions</td>
                                <td>Medium</td>
                            </tr>
                            <tr>
                                <td>MICE</td>
                                <td>MAR, need uncertainty</td>
                                <td>Large datasets</td>
                                <td>High</td>
                            </tr>
                            <tr>
                                <td>Random Forest</td>
                                <td>Non-linear patterns</td>
                                <td>Small datasets</td>
                                <td>High</td>
                            </tr>
                            <tr>
                                <td>Deep Learning</td>
                                <td>Complex patterns, big data</td>
                                <td>&lt;10k samples</td>
                                <td>Very High</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

        </div>

        <!-- Implementation Section -->
        <div id="implementation" class="content-section">
            
            <div class="card">
                <h2 class="section-title">
                    <span class="icon-box">💻</span>
                    Complete Implementation Pipeline
                </h2>

                <div class="code-block">
# Complete Missing Value Imputation Pipeline
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.impute import KNNImputer, SimpleImputer, IterativeImputer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import seaborn as sns
import matplotlib.pyplot as plt

class MissingValueHandler:
    """
    Comprehensive missing value handling pipeline
    """
    
    def __init__(self, strategy='auto', threshold=0.5):
        """
        Parameters:
        -----------
        strategy : str, default='auto'
            'auto', 'simple', 'knn', 'iterative', 'domain'
        threshold : float, default=0.5
            Threshold for dropping columns with too many missing values
        """
        self.strategy = strategy
        self.threshold = threshold
        self.imputers = {}
        self.missing_report = {}
        
    def analyze_missing(self, df):
        """Analyze missing value patterns"""
        
        self.missing_report = {
            'total_missing': df.isnull().sum().sum(),
            'total_cells': df.size,
            'missing_percentage': (df.isnull().sum().sum() / df.size) * 100,
            'columns': {}
        }
        
        for col in df.columns:
            missing_count = df[col].isnull().sum()
            missing_pct = (missing_count / len(df)) * 100
            
            self.missing_report['columns'][col] = {
                'missing_count': missing_count,
                'missing_percentage': missing_pct,
                'dtype': str(df[col].dtype),
                'unique_values': df[col].nunique(),
                'recommendation': self._get_recommendation(df[col], missing_pct)
            }
        
        return self.missing_report
    
    def _get_recommendation(self, series, missing_pct):
        """Get imputation recommendation for a column"""
        
        if missing_pct > self.threshold * 100:
            return 'DROP'
        elif missing_pct == 0:
            return 'NO_ACTION'
        elif series.dtype in ['object', 'category']:
            return 'MODE'
        elif missing_pct < 5:
            return 'SIMPLE'
        elif series.nunique() / len(series) > 0.9:  # High cardinality
            return 'KNN'
        else:
            return 'ITERATIVE'
    
    def visualize_missing(self, df):
        """Create missing value visualizations"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Missing value heatmap
        sns.heatmap(df.isnull(), cbar=True, cmap='RdYlGn_r', 
                   ax=axes[0, 0], yticklabels=False)
        axes[0, 0].set_title('Missing Value Patterns')
        
        # 2. Missing value bar plot
        missing_counts = df.isnull().sum()
        missing_counts[missing_counts > 0].plot(kind='bar', ax=axes[0, 1])
        axes[0, 1].set_title('Missing Values by Column')
        axes[0, 1].set_ylabel('Count')
        
        # 3. Missing value percentage
        missing_pct = (df.isnull().sum() / len(df)) * 100
        missing_pct[missing_pct > 0].plot(kind='barh', ax=axes[1, 0])
        axes[1, 0].set_title('Missing Percentage by Column')
        axes[1, 0].set_xlabel('Percentage (%)')
        
        # 4. Correlation of missingness
        missing_df = df.isnull().astype(int)
        corr_matrix = missing_df.corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', 
                   center=0, ax=axes[1, 1], fmt='.2f')
        axes[1, 1].set_title('Correlation of Missingness')
        
        plt.tight_layout()
        plt.show()
    
    def fit_transform(self, df):
        """Fit and transform the data with appropriate imputation"""
        
        df_imputed = df.copy()
        
        # Analyze missing patterns
        self.analyze_missing(df)
        
        # Drop columns with too many missing values
        cols_to_drop = [col for col, info in self.missing_report['columns'].items()
                       if info['recommendation'] == 'DROP']
        if cols_to_drop:
            print(f"Dropping columns with >{self.threshold*100}% missing: {cols_to_drop}")
            df_imputed = df_imputed.drop(columns=cols_to_drop)
        
        # Separate numerical and categorical columns
        numerical_cols = df_imputed.select_dtypes(include=['number']).columns.tolist()
        categorical_cols = df_imputed.select_dtypes(include=['object', 'category']).columns.tolist()
        
        if self.strategy == 'auto':
            # Use mixed strategy based on analysis
            for col in df_imputed.columns:
                if df_imputed[col].isnull().sum() == 0:
                    continue
                    
                recommendation = self.missing_report['columns'][col]['recommendation']
                
                if recommendation == 'MODE' or col in categorical_cols:
                    self.imputers[col] = SimpleImputer(strategy='most_frequent')
                elif recommendation == 'SIMPLE':
                    self.imputers[col] = SimpleImputer(strategy='median')
                elif recommendation == 'KNN':
                    self.imputers[col] = KNNImputer(n_neighbors=5)
                else:  # ITERATIVE
                    self.imputers[col] = IterativeImputer(random_state=42)
                
                # Fit and transform
                if col in categorical_cols:
                    df_imputed[col] = self.imputers[col].fit_transform(
                        df_imputed[[col]]
                    ).ravel()
                else:
                    df_imputed[col] = self.imputers[col].fit_transform(
                        df_imputed[[col]]
                    ).ravel()
        
        elif self.strategy == 'simple':
            # Simple imputation for all
            num_imputer = SimpleImputer(strategy='median')
            cat_imputer = SimpleImputer(strategy='most_frequent')
            
            if numerical_cols:
                df_imputed[numerical_cols] = num_imputer.fit_transform(
                    df_imputed[numerical_cols]
                )
            if categorical_cols:
                df_imputed[categorical_cols] = cat_imputer.fit_transform(
                    df_imputed[categorical_cols]
                )
        
        elif self.strategy == 'knn':
            # KNN for numerical only
            if numerical_cols:
                knn_imputer = KNNImputer(n_neighbors=5)
                df_imputed[numerical_cols] = knn_imputer.fit_transform(
                    df_imputed[numerical_cols]
                )
            if categorical_cols:
                cat_imputer = SimpleImputer(strategy='most_frequent')
                df_imputed[categorical_cols] = cat_imputer.fit_transform(
                    df_imputed[categorical_cols]
                )
        
        elif self.strategy == 'iterative':
            # Iterative imputation
            if numerical_cols:
                iter_imputer = IterativeImputer(random_state=42)
                df_imputed[numerical_cols] = iter_imputer.fit_transform(
                    df_imputed[numerical_cols]
                )
            if categorical_cols:
                cat_imputer = SimpleImputer(strategy='most_frequent')
                df_imputed[categorical_cols] = cat_imputer.fit_transform(
                    df_imputed[categorical_cols]
                )
        
        return df_imputed
    
    def transform(self, df):
        """Transform new data using fitted imputers"""
        
        df_imputed = df.copy()
        
        for col, imputer in self.imputers.items():
            if col in df_imputed.columns and df_imputed[col].isnull().sum() > 0:
                df_imputed[col] = imputer.transform(df_imputed[[col]]).ravel()
        
        return df_imputed

# EXAMPLE USAGE WITH REAL DATASET
print("="*60)
print("TITANIC DATASET EXAMPLE")
print("="*60)

# Load Titanic dataset (you can use any dataset)
# Creating a sample similar to Titanic
np.random.seed(42)
n = 891

titanic = pd.DataFrame({
    'Survived': np.random.binomial(1, 0.38, n),
    'Pclass': np.random.choice([1, 2, 3], n, p=[0.24, 0.21, 0.55]),
    'Sex': np.random.choice(['male', 'female'], n, p=[0.65, 0.35]),
    'Age': np.random.normal(30, 14, n),
    'SibSp': np.random.poisson(0.5, n),
    'Parch': np.random.poisson(0.4, n),
    'Fare': np.random.exponential(32, n),
    'Embarked': np.random.choice(['C', 'Q', '

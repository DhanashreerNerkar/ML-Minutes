<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformers in NLP Tasks</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
            color: #333;
        }
        
        .container {
            max-width: 1400px;
            margin: 0 auto;
        }
        
        h1 {
            text-align: center;
            color: white;
            font-size: 2.8em;
            margin-bottom: 10px;
            text-shadow: 3px 3px 6px rgba(0,0,0,0.3);
        }
        
        .subtitle {
            text-align: center;
            color: rgba(255, 255, 255, 0.95);
            font-size: 1.2em;
            margin-bottom: 30px;
        }
        
        .card {
            background: white;
            border-radius: 20px;
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            animation: slideIn 0.5s ease-out;
        }
        
        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
        
        .task-selector {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        
        .task-button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 15px 25px;
            border-radius: 25px;
            font-size: 1em;
            cursor: pointer;
            transition: all 0.3s;
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }
        
        .task-button:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(0,0,0,0.3);
        }
        
        .task-button.active {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            transform: scale(1.1);
        }
        
        .demo-area {
            background: #f8f9fa;
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
            min-height: 300px;
        }
        
        .input-area {
            margin-bottom: 20px;
        }
        
        .text-input {
            width: 100%;
            padding: 15px;
            border: 2px solid #667eea;
            border-radius: 10px;
            font-size: 1.1em;
            transition: border-color 0.3s;
        }
        
        .text-input:focus {
            outline: none;
            border-color: #764ba2;
            box-shadow: 0 0 10px rgba(118, 75, 162, 0.2);
        }
        
        .process-button {
            background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
            color: #333;
            border: none;
            padding: 12px 30px;
            border-radius: 20px;
            font-size: 1em;
            font-weight: bold;
            cursor: pointer;
            margin-top: 15px;
            transition: all 0.3s;
        }
        
        .process-button:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 20px rgba(67, 233, 123, 0.4);
        }
        
        .visualization-container {
            display: grid;
            grid-template-columns: 1fr 2fr 1fr;
            gap: 20px;
            margin: 30px 0;
            align-items: start;
        }
        
        @media (max-width: 968px) {
            .visualization-container {
                grid-template-columns: 1fr;
            }
        }
        
        .component-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
            transition: all 0.3s;
        }
        
        .component-box.active {
            animation: pulse 2s infinite;
        }
        
        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.05); }
        }
        
        .attention-matrix {
            background: white;
            border-radius: 10px;
            padding: 15px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }
        
        .matrix-grid {
            display: grid;
            gap: 3px;
            margin-top: 10px;
        }
        
        .matrix-cell {
            aspect-ratio: 1;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.7em;
            border-radius: 3px;
            transition: all 0.3s;
        }
        
        .result-display {
            background: linear-gradient(135deg, #c3cfe2 0%, #f5f7fa 100%);
            border-radius: 10px;
            padding: 20px;
            margin-top: 20px;
            border-left: 4px solid #667eea;
        }
        
        .token-display {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
            margin: 15px 0;
        }
        
        .token {
            background: white;
            border: 2px solid #667eea;
            padding: 8px 15px;
            border-radius: 20px;
            font-weight: bold;
            transition: all 0.3s;
            position: relative;
        }
        
        .token.highlighted {
            background: linear-gradient(135deg, #feca57 0%, #ff6b6b 100%);
            color: white;
            border-color: #ff6b6b;
            transform: scale(1.1);
        }
        
        .token-score {
            position: absolute;
            top: -10px;
            right: -5px;
            background: #ff6b6b;
            color: white;
            padding: 2px 6px;
            border-radius: 10px;
            font-size: 0.7em;
        }
        
        .architecture-flow {
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin: 30px 0;
            flex-wrap: wrap;
            gap: 20px;
        }
        
        .flow-step {
            flex: 1;
            min-width: 150px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 15px;
            text-align: center;
            position: relative;
            transition: all 0.3s;
        }
        
        .flow-step:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
        }
        
        .flow-arrow {
            position: absolute;
            right: -20px;
            top: 50%;
            transform: translateY(-50%);
            font-size: 2em;
            color: #667eea;
        }
        
        .nlp-task-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .task-card {
            background: linear-gradient(135deg, #fff 0%, #f0f0f0 100%);
            border-radius: 15px;
            padding: 20px;
            border: 2px solid transparent;
            transition: all 0.3s;
            cursor: pointer;
        }
        
        .task-card:hover {
            border-color: #667eea;
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0,0,0,0.15);
        }
        
        .task-icon {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        
        .highlight-box {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }
        
        .explanation-panel {
            background: #e3f2fd;
            border-radius: 10px;
            padding: 20px;
            margin-top: 20px;
        }
        
        .performance-chart {
            display: flex;
            align-items: flex-end;
            justify-content: space-around;
            height: 200px;
            margin: 20px 0;
        }
        
        .bar {
            width: 60px;
            background: linear-gradient(to top, #667eea, #764ba2);
            border-radius: 5px 5px 0 0;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: flex-end;
            padding-bottom: 10px;
            color: white;
            font-weight: bold;
            transition: all 0.3s;
        }
        
        .bar:hover {
            transform: scaleY(1.1);
        }
        
        .bar-label {
            margin-top: 10px;
            color: #333;
            font-size: 0.9em;
        }
        
        .model-comparison {
            display: flex;
            justify-content: space-around;
            gap: 20px;
            margin: 30px 0;
            flex-wrap: wrap;
        }
        
        .model-card {
            flex: 1;
            min-width: 250px;
            background: white;
            border-radius: 15px;
            padding: 20px;
            text-align: center;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            transition: all 0.3s;
        }
        
        .model-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
        }
        
        .model-badge {
            display: inline-block;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 0.9em;
            margin-top: 10px;
        }
        
        @keyframes processing {
            0% { transform: translateX(0); }
            50% { transform: translateX(10px); }
            100% { transform: translateX(0); }
        }
        
        .processing-indicator {
            display: none;
            text-align: center;
            margin: 20px 0;
        }
        
        .processing-indicator.active {
            display: block;
        }
        
        .dot {
            display: inline-block;
            width: 10px;
            height: 10px;
            background: #667eea;
            border-radius: 50%;
            margin: 0 5px;
            animation: bounce 1.4s infinite ease-in-out both;
        }
        
        .dot:nth-child(1) { animation-delay: -0.32s; }
        .dot:nth-child(2) { animation-delay: -0.16s; }
        
        @keyframes bounce {
            0%, 80%, 100% {
                transform: scale(0);
            } 40% {
                transform: scale(1);
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>🚀 Transformers in NLP Tasks</h1>
        <p class="subtitle">See how attention mechanisms revolutionize language understanding</p>
        
        <!-- Core Architecture Overview -->
        <div class="card">
            <h2 style="color: #667eea; margin-bottom: 20px;">🏗️ How Transformers Process Language</h2>
            
            <div class="architecture-flow">
                <div class="flow-step">
                    <h3>📝 Input</h3>
                    <p>Text/Tokens</p>
                    <div class="flow-arrow">→</div>
                </div>
                <div class="flow-step">
                    <h3>➕ Position</h3>
                    <p>Add Order Info</p>
                    <div class="flow-arrow">→</div>
                </div>
                <div class="flow-step">
                    <h3>🎯 Attention</h3>
                    <p>Find Relations</p>
                    <div class="flow-arrow">→</div>
                </div>
                <div class="flow-step">
                    <h3>🔄 Encode</h3>
                    <p>Deep Understanding</p>
                    <div class="flow-arrow">→</div>
                </div>
                <div class="flow-step">
                    <h3>📤 Output</h3>
                    <p>Task Result</p>
                </div>
            </div>
            
            <div class="highlight-box">
                <h3>🔑 Key Innovation: Parallel Processing</h3>
                <p><strong>RNN/LSTM:</strong> Reads "The cat sat on the mat" word by word → Takes 6 steps</p>
                <p><strong>Transformer:</strong> Sees entire sentence at once → Takes 1 step!</p>
                <p>Result: 100x faster training, better long-range understanding</p>
            </div>
        </div>
        
        <!-- Interactive NLP Tasks Demo -->
        <div class="card">
            <h2 style="color: #667eea; margin-bottom: 20px;">🎯 Try Different NLP Tasks</h2>
            
            <div class="task-selector">
                <button class="task-button active" onclick="selectTask('translation')">🌍 Translation</button>
                <button class="task-button" onclick="selectTask('sentiment')">😊 Sentiment Analysis</button>
                <button class="task-button" onclick="selectTask('ner')">🏷️ Named Entity Recognition</button>
                <button class="task-button" onclick="selectTask('qa')">❓ Question Answering</button>
                <button class="task-button" onclick="selectTask('summarization')">📋 Summarization</button>
            </div>
            
            <div class="demo-area" id="demo-area">
                <div id="translation-demo" class="task-demo">
                    <h3>Translation Task (Encoder-Decoder)</h3>
                    <div class="input-area">
                        <input type="text" class="text-input" id="translation-input" 
                               value="The weather is beautiful today" placeholder="Enter English text...">
                        <button class="process-button" onclick="processTranslation()">Translate to French →</button>
                    </div>
                    <div class="processing-indicator" id="translation-processing">
                        <p>Processing through Transformer layers</p>
                        <div><span class="dot"></span><span class="dot"></span><span class="dot"></span></div>
                    </div>
                    <div id="translation-result"></div>
                </div>
                
                <div id="sentiment-demo" class="task-demo" style="display: none;">
                    <h3>Sentiment Analysis (Encoder-only, like BERT)</h3>
                    <div class="input-area">
                        <input type="text" class="text-input" id="sentiment-input" 
                               value="This movie was absolutely fantastic!" placeholder="Enter text to analyze...">
                        <button class="process-button" onclick="processSentiment()">Analyze Sentiment →</button>
                    </div>
                    <div id="sentiment-result"></div>
                </div>
                
                <div id="ner-demo" class="task-demo" style="display: none;">
                    <h3>Named Entity Recognition (Token Classification)</h3>
                    <div class="input-area">
                        <input type="text" class="text-input" id="ner-input" 
                               value="Apple Inc. was founded by Steve Jobs in Cupertino" placeholder="Enter text...">
                        <button class="process-button" onclick="processNER()">Extract Entities →</button>
                    </div>
                    <div id="ner-result"></div>
                </div>
                
                <div id="qa-demo" class="task-demo" style="display: none;">
                    <h3>Question Answering (Encoder for context, Decoder for answer)</h3>
                    <div class="input-area">
                        <input type="text" class="text-input" id="qa-context" 
                               value="The Eiffel Tower is located in Paris, France. It was built in 1889." 
                               placeholder="Context...">
                        <input type="text" class="text-input" id="qa-question" 
                               value="When was the Eiffel Tower built?" 
                               placeholder="Question..." style="margin-top: 10px;">
                        <button class="process-button" onclick="processQA()">Get Answer →</button>
                    </div>
                    <div id="qa-result"></div>
                </div>
                
                <div id="summarization-demo" class="task-demo" style="display: none;">
                    <h3>Text Summarization (Encoder-Decoder)</h3>
                    <div class="input-area">
                        <textarea class="text-input" id="summary-input" rows="4"
                                  placeholder="Enter long text to summarize...">Artificial intelligence has made remarkable progress in recent years. Machine learning models can now understand and generate human language with unprecedented accuracy. These advances have led to applications in healthcare, education, and many other fields. The technology continues to evolve rapidly.</textarea>
                        <button class="process-button" onclick="processSummary()">Summarize →</button>
                    </div>
                    <div id="summary-result"></div>
                </div>
            </div>
        </div>
        
        <!-- Self-Attention Visualization -->
        <div class="card">
            <h2 style="color: #667eea; margin-bottom: 20px;">🔍 Self-Attention Mechanism in Action</h2>
            
            <div class="visualization-container">
                <div class="component-box">
                    <h3>Query (Q)</h3>
                    <p>"What am I looking for?"</p>
                    <small>Each word asks</small>
                </div>
                
                <div class="attention-matrix">
                    <h4 style="text-align: center;">Attention Scores Matrix</h4>
                    <div class="matrix-grid" id="attention-matrix" style="grid-template-columns: repeat(5, 1fr);">
                        <!-- Matrix will be populated by JavaScript -->
                    </div>
                    <p style="text-align: center; margin-top: 10px; font-size: 0.9em;">
                        Darker = Higher Attention
                    </p>
                </div>
                
                <div class="component-box">
                    <h3>Key (K) × Value (V)</h3>
                    <p>"Here's my information"</p>
                    <small>Each word provides</small>
                </div>
            </div>
            
            <div class="explanation-panel">
                <h3>Why Self-Attention Works for NLP:</h3>
                <ul style="line-height: 1.8;">
                    <li><strong>Contextual Understanding:</strong> "Bank" knows if it means financial or river based on surrounding words</li>
                    <li><strong>Long-range Dependencies:</strong> Can connect pronouns to their antecedents even 50 words apart</li>
                    <li><strong>Parallel Processing:</strong> All words processed simultaneously, not sequentially</li>
                    <li><strong>Dynamic Weights:</strong> Attention changes based on the specific input and task</li>
                </ul>
            </div>
        </div>
        
        <!-- Model Variants for Different Tasks -->
        <div class="card">
            <h2 style="color: #667eea; margin-bottom: 20px;">🎭 Transformer Variants for NLP Tasks</h2>
            
            <div class="model-comparison">
                <div class="model-card">
                    <h3 style="color: #f5576c;">BERT</h3>
                    <p style="font-size: 0.9em; color: #666;">Bidirectional Encoder</p>
                    <div style="margin: 20px 0;">
                        <div class="task-icon">🔍</div>
                        <p><strong>Best for Understanding:</strong></p>
                        <ul style="text-align: left; margin-top: 10px;">
                            <li>Classification</li>
                            <li>Named Entity Recognition</li>
                            <li>Question Answering</li>
                        </ul>
                    </div>
                    <div class="model-badge">340M parameters</div>
                </div>
                
                <div class="model-card">
                    <h3 style="color: #4facfe;">GPT</h3>
                    <p style="font-size: 0.9em; color: #666;">Autoregressive Decoder</p>
                    <div style="margin: 20px 0;">
                        <div class="task-icon">✍️</div>
                        <p><strong>Best for Generation:</strong></p>
                        <ul style="text-align: left; margin-top: 10px;">
                            <li>Text Completion</li>
                            <li>Story Writing</li>
                            <li>Code Generation</li>
                        </ul>
                    </div>
                    <div class="model-badge">175B parameters</div>
                </div>
                
                <div class="model-card">
                    <h3 style="color: #43e97b;">T5</h3>
                    <p style="font-size: 0.9em; color: #666;">Encoder-Decoder</p>
                    <div style="margin: 20px 0;">
                        <div class="task-icon">🔄</div>
                        <p><strong>Best for Transformation:</strong></p>
                        <ul style="text-align: left; margin-top: 10px;">
                            <li>Translation</li>
                            <li>Summarization</li>
                            <li>Any text-to-text</li>
                        </ul>
                    </div>
                    <div class="model-badge">11B parameters</div>
                </div>
            </div>
        </div>
        
        <!-- Performance Comparison -->
        <div class="card">
            <h2 style="color: #667eea; margin-bottom: 20px;">📊 Why Transformers Dominate NLP</h2>
            
            <h3>Performance on Common NLP Tasks:</h3>
            <div class="performance-chart">
                <div style="text-align: center;">
                    <div class="bar" style="height: 60%;">
                        <span>65%</span>
                    </div>
                    <div class="bar-label">RNN/LSTM</div>
                </div>
                <div style="text-align: center;">
                    <div class="bar" style="height: 75%;">
                        <span>78%</span>
                    </div>
                    <div class="bar-label">CNN</div>
                </div>
                <div style="text-align: center;">
                    <div class="bar" style="height: 95%; background: linear-gradient(to top, #f093fb, #f5576c);">
                        <span>94%</span>
                    </div>
                    <div class="bar-label">Transformer</div>
                </div>
            </div>
            
            <div class="nlp-task-grid">
                <div class="task-card" onclick="explainTask('translation')">
                    <div class="task-icon">🌐</div>
                    <h4>Machine Translation</h4>
                    <p><strong>BLEU Score:</strong> 41.8 (SOTA)</p>
                    <p>Captures context and idioms perfectly</p>
                </div>
                
                <div class="task-card" onclick="explainTask('qa')">
                    <div class="task-icon">❓</div>
                    <h4>Question Answering</h4>
                    <p><strong>F1 Score:</strong> 93.2 (SQuAD)</p>
                    <p>Understands complex reasoning</p>
                </div>
                
                <div class="task-card" onclick="explainTask('sentiment')">
                    <div class="task-icon">😊</div>
                    <h4>Sentiment Analysis</h4>
                    <p><strong>Accuracy:</strong> 96.4%</p>
                    <p>Detects subtle emotions and sarcasm</p>
                </div>
                
                <div class="task-card" onclick="explainTask('summarization')">
                    <div class="task-icon">📄</div>
                    <h4>Summarization</h4>
                    <p><strong>ROUGE Score:</strong> 47.2</p>
                    <p>Maintains key information</p>
                </div>
                
                <div class="task-card" onclick="explainTask('ner')">
                    <div class="task-icon">🏷️</div>
                    <h4>Named Entity Recognition</h4>
                    <p><strong>F1 Score:</strong> 92.8</p>
                    <p>Identifies entities in context</p>
                </div>
                
                <div class="task-card" onclick="explainTask('generation')">
                    <div class="task-icon">✏️</div>
                    <h4>Text Generation</h4>
                    <p><strong>Perplexity:</strong> 20.5</p>
                    <p>Human-like coherent text</p>
                </div>
            </div>
        </div>
        
        <!-- Key Concepts Summary -->
        <div class="card">
            <h2 style="color: #667eea; margin-bottom: 20px;">💡 Key Takeaways for NLP Tasks</h2>
            
            <div class="highlight-box">
                <h3>The Three Pillars of Transformer Success in NLP:</h3>
                
                <h4>1. Self-Attention for Context</h4>
                <p>Every word can directly attend to every other word, capturing complex relationships that RNNs miss.</p>
                
                <h4>2. Positional Encoding for Order</h4>
                <p>Sine/cosine functions encode word positions, maintaining sequence information without recurrence.</p>
                
                <h4>3. Parallelization for Speed</h4>
                <p>Process entire sequences simultaneously, making training 100x faster than sequential models.</p>
            </div>
            
            <div class="explanation-panel">
                <h3>🎯 Interview Answer Framework:</h3>
                <p><strong>Opening:</strong> "Transformers revolutionized NLP by replacing sequential processing with parallel self-attention mechanisms."</p>
                <br>
                <p><strong>Core Concept:</strong> "Instead of processing words one by one, Transformers use self-attention to let every word simultaneously look at and weigh the importance of every other word in the sequence."</p>
                <br>
                <p><strong>Components:</strong> "Three key innovations make this work: (1) Self-attention with Query-Key-Value matrices for finding relationships, (2) Positional encoding to maintain word order, and (3) Stacked encoder-decoder layers for deep understanding."</p>
                <br>
                <p><strong>Impact:</strong> "This architecture enables BERT to achieve 93% accuracy on question answering and GPT to generate human-like text, while training 100x faster than RNNs."</p>
            </div>
        </div>
    </div>
    
    <script>
        // Task Selection
        function selectTask(task) {
            // Update button states
            document.querySelectorAll('.task-button').forEach(btn => {
                btn.classList.remove('active');
            });
            event.target.classList.add('active');
            
            // Hide all demos
            document.querySelectorAll('.task-demo').forEach(demo => {
                demo.style.display = 'none';
            });
            
            // Show selected demo
            document.getElementById(task + '-demo').style.display = 'block';
        }
        
        // Translation Demo
        function processTranslation() {
            const input = document.getElementById('translation-input').value;
            const processing = document.getElementById('translation-processing');
            const resultDiv = document.getElementById('translation-result');
            
            processing.classList.add('active');
            resultDiv.innerHTML = '';
            
            setTimeout(() => {
                processing.classList.remove('active');
                
                // Simulated translation
                const tokens = input.split(' ');
                const translations = {
                    'The': 'Le', 'weather': 'temps', 'is': 'est', 
                    'beautiful': 'magnifique', 'today': "aujourd'hui"
                };
                
                resultDiv.innerHTML = `
                    <div class="result-display">
                        <h4>Encoder Output (Understanding):</h4>
                        <div class="token-display">
                            ${tokens.map(t => `<div class="token">${t}</div>`).join('')}
                        </div>
                        
                        <h4>Attention Weights:</h4>
                        <p>The model paid special attention to: 
                        <span class="token highlighted">beautiful</span> (context) and 
                        <span class="token highlighted">weather</span> (subject)</p>
                        
                        <h4>Decoder Output (Generation):</h4>
                        <div class="token-display">
                            ${tokens.map(t => `<div class="token" style="background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: white;">${translations[t] || t}</div>`).join('')}
                        </div>
                        
                        <p style="margin-top: 15px;"><strong>Final Translation:</strong> "Le temps est magnifique aujourd'hui"</p>
                    </div>
                `;
            }, 2000);
        }
        
        // Sentiment Analysis Demo
        function processSentiment() {
            const input = document.getElementById('sentiment-input').value;
            const resultDiv = document.getElementById('sentiment-result');
            
            // Simple sentiment detection
            const positive = ['fantastic', 'great', 'amazing', 'wonderful', 'excellent', 'love'];
            const negative = ['terrible', 'bad', 'awful', 'hate', 'horrible', 'worst'];
            
            let score = 0.5;
            positive.forEach(word => {
                if (input.toLowerCase().includes(word)) score += 0.3;
            });
            negative.forEach(word => {
                if (input.toLowerCase().includes(word)) score -= 0.3;
            });
            score = Math.max(0, Math.min(1, score));
            
            const sentiment = score > 0.6 ? 'Positive' : score < 0.4 ? 'Negative' : 'Neutral';
            const emoji = score > 0.6 ? '😊' : score < 0.4 ? '😞' : '😐';
            
            resultDiv.innerHTML = `
                <div class="result-display">
                    <h4>BERT Encoding Process:</h4>
                    <p>1. Tokenization: [CLS] ${input.split(' ').join(' ')} [SEP]</p>
                    <p>2. Self-attention finds emotional keywords</p>
                    <p>3. Pooler layer creates sentence representation</p>
                    
                    <h4>Classification Result:</h4>
                    <div style="font-size: 3em; text-align: center; margin: 20px 0;">${emoji}</div>
                    <p style="text-align: center; font-size: 1.5em;"><strong>${sentiment}</strong></p>
                    <p style="text-align: center;">Confidence: ${(score * 100).toFixed(1)}%</p>
                </div>
            `;
        }
        
        // NER Demo
        function processNER() {
            const input = document.getElementById('ner-input').value;
            const resultDiv = document.getElementById('ner-result');
            
            // Simple NER simulation
            const entities = [
                { text: 'Apple Inc.', type: 'ORG', color: '#4CAF50' },
                { text: 'Steve Jobs', type: 'PERSON', color: '#2196F3' },
                { text: 'Cupertino', type: 'LOC', color: '#FF9800' }
            ];
            
            let htmlResult = input;
            entities.forEach(entity => {
                if (input.includes(entity.text)) {
                    htmlResult = htmlResult.replace(
                        entity.text, 
                        `<span class="token" style="background: ${entity.color}; color: white; display: inline-block; margin: 2px;">
                            ${entity.text}
                            <span class="token-score">${entity.type}</span>
                        </span>`
                    );
                }
            });
            
            resultDiv.innerHTML = `
                <div class="result-display">
                    <h4>Token Classification Results:</h4>
                    <p>${htmlResult}</p>
                    
                    <h4>Entities Found:</h4>
                    <ul>
                        ${entities.filter(e => input.includes(e.text))
                            .map(e => `<li><strong>${e.type}:</strong> ${e.text}</li>`).join('')}
                    </ul>
                    
                    <p style="margin-top: 15px;"><em>Each token is classified using BERT's contextual embeddings</em></p>
                </div>
            `;
        }
        
        // QA Demo
        function processQA() {
            const context = document.getElementById('qa-context').value;
            const question = document.getElementById('qa-question').value;
            const resultDiv = document.getElementById('qa-result');
            
            // Simple QA simulation
            let answer = "Not found";
            if (question.includes('When') && context.includes('1889')) {
                answer = "1889";
            } else if (question.includes('Where') && context.includes('Paris')) {
                answer = "Paris, France";
            }
            
            resultDiv.innerHTML = `
                <div class="result-display">
                    <h4>Question Answering Process:</h4>
                    <p>1. Encode context with BERT</p>
                    <p>2. Encode question separately</p>
                    <p>3. Use attention to find relevant span in context</p>
                    <p>4. Extract answer tokens</p>
                    
                    <h4>Answer:</h4>
                    <p style="font-size: 1.5em; font-weight: bold; color: #667eea;">${answer}</p>
                    
                    <p style="margin-top: 15px;"><em>The model identified the answer span using start/end position predictions</em></p>
                </div>
            `;
        }
        
        // Summarization Demo
        function processSummary() {
            const input = document.getElementById('summary-input').value;
            const resultDiv = document.getElementById('summary-result');
            
            // Simple summary (take first sentence + key point)
            const sentences = input.match(/[^.!?]+[.!?]+/g) || [];
            const summary = sentences.length > 0 ? sentences[0].trim() : input.substring(0, 50) + '...';
            
            resultDiv.innerHTML = `
                <div class="result-display">
                    <h4>T5 Summarization Process:</h4>
                    <p>1. Encoder processes full text</p>
                    <p>2. Attention identifies key information</p>
                    <p>3. Decoder generates concise summary</p>
                    
                    <h4>Summary:</h4>
                    <p style="font-style: italic; padding: 15px; background: #f0f0f0; border-radius: 10px;">
                        "${summary}"
                    </p>
                    
                    <p style="margin-top: 15px;">Compression ratio: ${((summary.length / input.length) * 100).toFixed(1)}%</p>
                </div>
            `;
        }
        
        // Initialize attention matrix visualization
        function initAttentionMatrix() {
            const matrix = document.getElementById('attention-matrix');
            const words = ['The', 'cat', 'sat', 'on', 'mat'];
            
            // Sample attention scores
            const scores = [
                [0.8, 0.1, 0.05, 0.03, 0.02],
                [0.1, 0.6, 0.2, 0.05, 0.05],
                [0.05, 0.3, 0.5, 0.1, 0.05],
                [0.02, 0.05, 0.1, 0.7, 0.13],
                [0.03, 0.05, 0.1, 0.3, 0.52]
            ];
            
            scores.forEach((row, i) => {
                row.forEach((score, j) => {
                    const cell = document.createElement('div');
                    cell.className = 'matrix-cell';
                    cell.style.background = `rgba(102, 126, 234, ${score})`;
                    cell.style.color = score > 0.5 ? 'white' : '#333';
                    cell.textContent = score.toFixed(2);
                    cell.title = `${words[i]} → ${words[j]}`;
                    matrix.appendChild(cell);
                });
            });
        }
        
        // Initialize on load
        document.addEventListener('DOMContentLoaded', function() {
            initAttentionMatrix();
        });
        
        // Explain specific tasks
        function explainTask(task) {
            alert(`${task.charAt(0).toUpperCase() + task.slice(1)} uses Transformers to achieve state-of-the-art performance by leveraging self-attention to understand context and relationships in text.`);
        }
    </script>
</body>
</html>

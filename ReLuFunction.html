<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ReLU Function Explained</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            line-height: 1.6;
            min-height: 100vh;
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
        }
        
        .header {
            text-align: center;
            margin-bottom: 40px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 20px;
            padding: 40px;
            backdrop-filter: blur(15px);
        }
        
        .main-title {
            font-size: 3.5em;
            font-weight: bold;
            margin-bottom: 15px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
            background: linear-gradient(45deg, #ffd700, #ff6b6b);
            background-size: 200% 200%;
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            animation: gradient 3s ease infinite;
        }
        
        .subtitle {
            font-size: 1.3em;
            opacity: 0.9;
            margin-bottom: 20px;
        }
        
        .formula-display {
            background: rgba(0, 0, 0, 0.4);
            border-radius: 15px;
            padding: 20px;
            font-size: 1.8em;
            font-family: 'Courier New', monospace;
            text-align: center;
            margin: 20px 0;
            border: 2px solid rgba(255, 215, 0, 0.3);
        }
        
        .content-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 30px;
            margin: 40px 0;
        }
        
        .section-card {
            background: rgba(255, 255, 255, 0.15);
            border-radius: 20px;
            padding: 30px;
            backdrop-filter: blur(15px);
            border: 1px solid rgba(255, 255, 255, 0.2);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        
        .section-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 35px rgba(0, 0, 0, 0.3);
        }
        
        .section-title {
            font-size: 1.8em;
            font-weight: bold;
            margin-bottom: 20px;
            color: #ffd700;
            display: flex;
            align-items: center;
            gap: 15px;
        }
        
        .section-icon {
            font-size: 2em;
        }
        
        .graph-container {
            background: white;
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
            position: relative;
            overflow: hidden;
        }
        
        .graph-canvas {
            width: 100%;
            height: 300px;
            border: none;
        }
        
        .interactive-demo {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
            text-align: center;
        }
        
        .demo-title {
            font-size: 1.3em;
            font-weight: bold;
            margin-bottom: 20px;
            color: #ffd700;
        }
        
        .input-slider {
            width: 100%;
            margin: 20px 0;
            height: 8px;
            border-radius: 5px;
            background: rgba(255, 255, 255, 0.3);
            outline: none;
            -webkit-appearance: none;
        }
        
        .input-slider::-webkit-slider-thumb {
            -webkit-appearance: none;
            appearance: none;
            width: 25px;
            height: 25px;
            border-radius: 50%;
            background: #ffd700;
            cursor: pointer;
            box-shadow: 0 0 10px rgba(255, 215, 0, 0.5);
        }
        
        .result-display {
            background: rgba(0, 0, 0, 0.4);
            border-radius: 10px;
            padding: 20px;
            font-size: 1.5em;
            font-family: 'Courier New', monospace;
            margin: 15px 0;
        }
        
        .comparison-table {
            background: white;
            color: #333;
            border-radius: 15px;
            overflow: hidden;
            margin: 20px 0;
        }
        
        .table-header {
            background: linear-gradient(45deg, #667eea, #764ba2);
            color: white;
            padding: 15px;
            font-weight: bold;
            text-align: center;
        }
        
        .table-row {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            border-bottom: 1px solid #eee;
        }
        
        .table-cell {
            padding: 15px;
            text-align: center;
            border-right: 1px solid #eee;
        }
        
        .table-cell:last-child {
            border-right: none;
        }
        
        .code-example {
            background: rgba(0, 0, 0, 0.6);
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            border: 2px solid rgba(78, 205, 196, 0.3);
            overflow-x: auto;
        }
        
        .code-title {
            color: #4ecdc4;
            font-size: 1.1em;
            margin-bottom: 15px;
            font-weight: bold;
        }
        
        .code-line {
            margin: 8px 0;
            padding-left: 20px;
        }
        
        .code-comment {
            color: #a8e6cf;
            font-style: italic;
        }
        
        .keyword {
            color: #ff6b6b;
        }
        
        .string {
            color: #ffd700;
        }
        
        .number {
            color: #4ecdc4;
        }
        
        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }
        
        .pros, .cons {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            padding: 20px;
        }
        
        .pros {
            border-left: 5px solid #4ecdc4;
        }
        
        .cons {
            border-left: 5px solid #ff6b6b;
        }
        
        .pros-title {
            color: #4ecdc4;
            font-weight: bold;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .cons-title {
            color: #ff6b6b;
            font-weight: bold;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .point {
            margin: 10px 0;
            padding-left: 20px;
            position: relative;
        }
        
        .point::before {
            content: "•";
            position: absolute;
            left: 0;
            color: currentColor;
        }
        
        .variants-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .variant-card {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            padding: 20px;
            text-align: center;
            transition: transform 0.3s ease;
        }
        
        .variant-card:hover {
            transform: scale(1.05);
        }
        
        .variant-title {
            font-size: 1.2em;
            font-weight: bold;
            color: #ffd700;
            margin-bottom: 10px;
        }
        
        .variant-formula {
            background: rgba(0, 0, 0, 0.4);
            border-radius: 8px;
            padding: 10px;
            font-family: 'Courier New', monospace;
            margin: 10px 0;
            font-size: 0.9em;
        }
        
        .tip-box {
            background: linear-gradient(45deg, rgba(78, 205, 196, 0.3), rgba(68, 160, 141, 0.3));
            border-radius: 15px;
            padding: 25px;
            margin: 30px 0;
            border: 2px solid rgba(78, 205, 196, 0.5);
        }
        
        .tip-title {
            font-size: 1.3em;
            font-weight: bold;
            color: #4ecdc4;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .animation-demo {
            text-align: center;
            margin: 30px 0;
        }
        
        .neuron-visual {
            display: inline-block;
            width: 80px;
            height: 80px;
            border-radius: 50%;
            background: linear-gradient(45deg, #667eea, #764ba2);
            margin: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.2em;
            font-weight: bold;
            transition: all 0.3s ease;
            cursor: pointer;
        }
        
        .neuron-visual:hover {
            transform: scale(1.1);
            box-shadow: 0 0 20px rgba(255, 215, 0, 0.5);
        }
        
        .arrow {
            font-size: 2em;
            margin: 0 20px;
            color: #ffd700;
        }
        
        .network-flow {
            display: flex;
            align-items: center;
            justify-content: center;
            flex-wrap: wrap;
            gap: 20px;
            margin: 30px 0;
        }
        
        @keyframes gradient {
            0% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
            100% { background-position: 0% 50%; }
        }
        
        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.05); }
        }
        
        .pulse-animation {
            animation: pulse 2s ease-in-out infinite;
        }
        
        @media (max-width: 768px) {
            .main-title {
                font-size: 2.5em;
            }
            
            .content-grid {
                grid-template-columns: 1fr;
            }
            
            .pros-cons {
                grid-template-columns: 1fr;
            }
            
            .table-row {
                grid-template-columns: 1fr;
            }
            
            .network-flow {
                flex-direction: column;
            }
            
            .arrow {
                transform: rotate(90deg);
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <div class="main-title">ReLU Function</div>
            <div class="subtitle">The Game-Changing Activation Function in Deep Learning</div>
            <div class="formula-display">
                f(x) = max(0, x) = { x if x > 0, 0 if x ≤ 0 }
            </div>
            <div style="margin-top: 20px; font-size: 1.1em;">
                <strong>Simple Rule:</strong> If input is positive → keep it. If negative → make it zero.
            </div>
        </div>
        
        <div class="content-grid">
            <!-- Visual Graph -->
            <div class="section-card">
                <div class="section-title">
                    <span class="section-icon">📈</span>
                    Visual Representation
                </div>
                <div class="graph-container">
                    <canvas id="reluGraph" class="graph-canvas"></canvas>
                </div>
                <div style="text-align: center; margin-top: 15px; color: #333;">
                    <strong>Key Features:</strong><br>
                    🔴 Red line shows negative inputs become 0<br>
                    🔵 Blue line shows positive inputs stay the same
                </div>
            </div>
            
            <!-- Interactive Demo -->
            <div class="section-card">
                <div class="section-title">
                    <span class="section-icon">🎮</span>
                    Interactive Demo
                </div>
                <div class="interactive-demo">
                    <div class="demo-title">Try Different Input Values</div>
                    <div style="margin: 20px 0;">
                        <label for="inputSlider" style="display: block; margin-bottom: 10px;">
                            Input Value: <span id="inputValue">0</span>
                        </label>
                        <input type="range" id="inputSlider" class="input-slider" 
                               min="-10" max="10" step="0.1" value="0">
                    </div>
                    <div class="result-display">
                        ReLU(<span id="inputDisplay">0</span>) = <span id="outputDisplay">0</span>
                    </div>
                    <div style="margin-top: 20px;">
                        <div id="reluExplanation">When input = 0, ReLU output = 0</div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Why ReLU is Important -->
        <div class="section-card">
            <div class="section-title">
                <span class="section-icon">⚡</span>
                Why ReLU Revolutionized Deep Learning
            </div>
            <div class="pros-cons">
                <div class="pros">
                    <div class="pros-title">
                        <span>✅</span>
                        Advantages
                    </div>
                    <div class="point">Computationally efficient (just max operation)</div>
                    <div class="point">Solves vanishing gradient problem</div>
                    <div class="point">Sparse activation (many neurons output 0)</div>
                    <div class="point">No saturation for positive values</div>
                    <div class="point">Helps networks train faster and deeper</div>
                </div>
                <div class="cons">
                    <div class="cons-title">
                        <span>❌</span>
                        Disadvantages
                    </div>
                    <div class="point">Dying ReLU problem (neurons can get stuck at 0)</div>
                    <div class="point">Not zero-centered (can cause zigzag dynamics)</div>
                    <div class="point">No gradient for negative inputs</div>
                    <div class="point">Can be sensitive to learning rate</div>
                </div>
            </div>
        </div>
        
        <!-- Code Examples -->
        <div class="content-grid">
            <div class="section-card">
                <div class="section-title">
                    <span class="section-icon">💻</span>
                    Implementation
                </div>
                <div class="code-example">
                    <div class="code-title">Python Implementation</div>
                    <div class="code-line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
                    <div class="code-line"></div>
                    <div class="code-line"><span class="code-comment"># Simple ReLU function</span></div>
                    <div class="code-line"><span class="keyword">def</span> relu(x):</div>
                    <div class="code-line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</div>
                    <div class="code-line"></div>
                    <div class="code-line"><span class="code-comment"># Example usage</span></div>
                    <div class="code-line">inputs = np.array([<span class="number">-3</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">5</span>])</div>
                    <div class="code-line">outputs = relu(inputs)</div>
                    <div class="code-line">print(outputs)  <span class="code-comment"># [0 0 0 2 5]</span></div>
                </div>
                
                <div class="code-example">
                    <div class="code-title">TensorFlow/Keras</div>
                    <div class="code-line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div>
                    <div class="code-line"></div>
                    <div class="code-line"><span class="code-comment"># In a neural network layer</span></div>
                    <div class="code-line">model = tf.keras.Sequential([</div>
                    <div class="code-line">    tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</div>
                    <div class="code-line">    tf.keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>),</div>
                    <div class="code-line">    tf.keras.layers.Dense(<span class="number">10</span>)</div>
                    <div class="code-line">])</div>
                </div>
            </div>
            
            <div class="section-card">
                <div class="section-title">
                    <span class="section-icon">🔄</span>
                    ReLU Variants
                </div>
                <div class="variants-grid">
                    <div class="variant-card">
                        <div class="variant-title">Leaky ReLU</div>
                        <div class="variant-formula">f(x) = max(αx, x)</div>
                        <div>Allows small negative values (α = 0.01)</div>
                    </div>
                    <div class="variant-card">
                        <div class="variant-title">ELU</div>
                        <div class="variant-formula">f(x) = x if x>0<br>α(e^x - 1) if x≤0</div>
                        <div>Smooth curve for negative values</div>
                    </div>
                    <div class="variant-card">
                        <div class="variant-title">Swish</div>
                        <div class="variant-formula">f(x) = x × sigmoid(x)</div>
                        <div>Self-gated, smooth everywhere</div>
                    </div>
                    <div class="variant-card">
                        <div class="variant-title">GELU</div>
                        <div class="variant-formula">f(x) = x × Φ(x)</div>
                        <div>Used in modern transformers</div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Neural Network Visualization -->
        <div class="section-card">
            <div class="section-title">
                <span class="section-icon">🧠</span>
                ReLU in Neural Networks
            </div>
            <div class="animation-demo">
                <div style="margin-bottom: 30px; font-size: 1.1em;">
                    <strong>How ReLU works in a neural network:</strong>
                </div>
                <div class="network-flow">
                    <div class="neuron-visual pulse-animation">-2.5</div>
                    <div class="arrow">→</div>
                    <div class="neuron-visual" style="background: #ff6b6b;">ReLU</div>
                    <div class="arrow">→</div>
                    <div class="neuron-visual">0</div>
                </div>
                <div class="network-flow">
                    <div class="neuron-visual pulse-animation">3.7</div>
                    <div class="arrow">→</div>
                    <div class="neuron-visual" style="background: #4ecdc4;">ReLU</div>
                    <div class="arrow">→</div>
                    <div class="neuron-visual">3.7</div>
                </div>
                <div style="margin-top: 30px; background: rgba(255,255,255,0.1); padding: 20px; border-radius: 10px;">
                    <strong>Key Insight:</strong> ReLU creates sparsity in the network. About 50% of neurons will output 0, 
                    making the network more efficient and less prone to overfitting.
                </div>
            </div>
        </div>
        
        <!-- Comparison Table -->
        <div class="section-card">
            <div class="section-title">
                <span class="section-icon">⚖️</span>
                ReLU vs Other Activation Functions
            </div>
            <div class="comparison-table">
                <div class="table-header">Activation Function Comparison</div>
                <div class="table-row">
                    <div class="table-cell"><strong>Function</strong></div>
                    <div class="table-cell"><strong>Range</strong></div>
                    <div class="table-cell"><strong>Key Feature</strong></div>
                </div>
                <div class="table-row" style="background: #f8f9fa;">
                    <div class="table-cell"><strong>ReLU</strong></div>
                    <div class="table-cell">[0, ∞)</div>
                    <div class="table-cell">Simple, fast, sparse</div>
                </div>
                <div class="table-row">
                    <div class="table-cell">Sigmoid</div>
                    <div class="table-cell">(0, 1)</div>
                    <div class="table-cell">S-shaped, smooth</div>
                </div>
                <div class="table-row" style="background: #f8f9fa;">
                    <div class="table-cell">Tanh</div>
                    <div class="table-cell">(-1, 1)</div>
                    <div class="table-cell">Zero-centered</div>
                </div>
                <div class="table-row">
                    <div class="table-cell">Leaky ReLU</div>
                    <div class="table-cell">(-∞, ∞)</div>
                    <div class="table-cell">No dying neurons</div>
                </div>
            </div>
        </div>
        
        <!-- Pro Tips -->
        <div class="tip-box">
            <div class="tip-title">
                <span>💡</span>
                Pro Tips for Using ReLU
            </div>
            <div class="point"><strong>Initialize weights properly:</strong> Use He initialization for ReLU networks</div>
            <div class="point"><strong>Learning rate:</strong> Start with moderate learning rates (0.01-0.001)</div>
            <div class="point"><strong>Batch normalization:</strong> Helps stabilize ReLU training</div>
            <div class="point"><strong>Monitor dead neurons:</strong> If too many neurons die, try Leaky ReLU</div>
            <div class="point"><strong>Hidden layers only:</strong> Don't use ReLU in output layer for regression</div>
        </div>
        
        <!-- Real-world Impact -->
        <div class="section-card">
            <div class="section-title">
                <span class="section-icon">🌟</span>
                Real-world Impact
            </div>
            <div style="font-size: 1.1em; line-height: 1.8;">
                <p><strong>Before ReLU (pre-2010):</strong> Deep networks were hard to train due to vanishing gradients. 
                Most networks had only 2-3 hidden layers.</p>
                
                <p style="margin: 20px 0;"><strong>After ReLU:</strong> Enabled training of much deeper networks (50+ layers), 
                leading to breakthroughs in computer vision, NLP, and other AI fields.</p>
                
                <p><strong>Success Stories:</strong></p>
                <ul style="margin-left: 20px; margin-top: 10px;">
                    <li>AlexNet (2012) - First CNN to win ImageNet using ReLU</li>
                    <li>ResNet (2015) - 152 layers deep with ReLU</li>
                    <li>Most modern deep learning models still use ReLU variants</li>
                </ul>
            </div>
        </div>
    </div>

    <script>
        // Interactive slider functionality
        const inputSlider = document.getElementById('inputSlider');
        const inputValue = document.getElementById('inputValue');
        const inputDisplay = document.getElementById('inputDisplay');
        const outputDisplay = document.getElementById('outputDisplay');
        const reluExplanation = document.getElementById('reluExplanation');

        function updateDemo() {
            const input = parseFloat(inputSlider.value);
            const output = Math.max(0, input);
            
            inputValue.textContent = input.toFixed(1);
            inputDisplay.textContent = input.toFixed(1);
            outputDisplay.textContent = output.toFixed(1);
            
            if (input > 0) {
                reluExplanation.textContent = `Since ${input.toFixed(1)} > 0, ReLU outputs ${output.toFixed(1)} (unchanged)`;
                outputDisplay.style.color = '#4ecdc4';
            } else {
                reluExplanation.textContent = `Since ${input.toFixed(1)} ≤ 0, ReLU outputs 0 (negative inputs become 0)`;
                outputDisplay.style.color = '#ff6b6b';
            }
        }

        inputSlider.addEventListener('input', updateDemo);
        
        // Draw ReLU graph
        function drawReluGraph() {
            const canvas = document.getElementById('reluGraph');
            const ctx = canvas.getContext('2d');
            
            // Set canvas size
            canvas.width = canvas.offsetWidth * window.devicePixelRatio;
            canvas.height = canvas.offsetHeight * window.devicePixelRatio;
            ctx.scale(window.devicePixelRatio, window.devicePixelRatio);
            
            const width = canvas.offsetWidth;
            const height = canvas.offsetHeight;
            
            // Clear canvas
            ctx.fillStyle = '#f8f9fa';
            ctx.fillRect(0, 0, width, height);
            
            // Set up coordinate system
            const centerX = width / 2;
            const centerY = height / 2;
            const scale = 20;
            
            // Draw axes
            ctx.strokeStyle = '#666';
            ctx.lineWidth = 2;
            ctx.beginPath();
            // X-axis
            ctx.moveTo(0, centerY);
            ctx.lineTo(width, centerY);
            // Y-axis
            ctx.moveTo(centerX, 0);
            ctx.lineTo(centerX, height);
            ctx.stroke();
            
            // Draw axis labels
            ctx.fillStyle = '#333';
            ctx.font = '14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('0', centerX - 15, centerY + 15);
            ctx.fillText('x', width - 20, centerY + 15);
            ctx.save();
            ctx.translate(15, 20);
            ctx.rotate(-Math.PI / 2);
            ctx.fillText('f(x)', 0, 0);
            ctx.restore();
            
            // Draw grid
            ctx.strokeStyle = '#ddd';
            ctx.lineWidth = 1;
            ctx.beginPath();
            for (let i = -10; i <= 10; i++) {
                if (i !== 0) {
                    // Vertical lines
                    ctx.moveTo(centerX + i * scale, 0);
                    ctx.lineTo(centerX + i * scale, height);
                    // Horizontal lines
                    ctx.moveTo(0, centerY + i * scale);
                    ctx.lineTo(width, centerY + i * scale);
                }
            }
            ctx.stroke();
            
            // Draw ReLU function
            ctx.lineWidth = 3;
            
            // Negative part (y = 0)
            ctx.strokeStyle = '#ff6b6b';
            ctx.beginPath();
            ctx.moveTo(0, centerY);
            ctx.lineTo(centerX, centerY);
            ctx.stroke();
            
            // Positive part (y = x)
            ctx.strokeStyle = '#4ecdc4';
            ctx.beginPath();
            ctx.moveTo(centerX, centerY);
            ctx.lineTo(width, centerY - (width - centerX) / scale * scale);
            ctx.stroke();
            
            // Highlight the corner point
            ctx.fillStyle = '#ffd700';
            ctx.beginPath();
            ctx.arc(centerX, centerY, 6, 0, 2 * Math.PI);
            ctx.fill();
            ctx.strokeStyle = '#333';
            ctx.lineWidth = 2;
            ctx.stroke();
            
            // Add some sample points
            const samplePoints = [-3, -1, 1, 2, 3];
            samplePoints.forEach(x => {
                const y = Math.max(0, x);
                const canvasX = centerX + x * scale;
                const canvasY = centerY - y * scale;
                
                if (canvasX >= 0 && canvasX <= width && canvasY >= 0 && canvasY <= height) {
                    ctx.fillStyle = x <= 0 ? '#ff6b6b' : '#4ecdc4';
                    ctx.beginPath();
                    ctx.arc(canvasX, canvasY, 4, 0, 2 * Math.PI);
                    ctx.fill();
                    
                    // Label points
                    ctx.fillStyle = '#333';
                    ctx.font = '12px Arial';
                    ctx.textAlign = 'center';
                    ctx.fillText(`(${x},${y})`, canvasX, canvasY - 10);
                }
            });
        }
        
        // Initialize
        updateDemo();
        
        // Draw graph when page loads
        window.addEventListener('load', drawReluGraph);
        window.addEventListener('resize', drawReluGraph);
        
        // Add click animations to neurons
        document.querySelectorAll('.neuron-visual').forEach(neuron => {
            neuron.addEventListener('click', function() {
                this.style.transform = 'scale(1.2)';
                this.style.boxShadow = '0 0 25px rgba(255, 215, 0, 0.8)';
                setTimeout(() => {
                    this.style.transform = 'scale(1)';
                    this.style.boxShadow = 'none';
                }, 300);
            });
        });
    </script>
</body>
</html>
